{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates how to configure and train a reinforcement learning agent for statistical arbitrage using a snapshot model. The agent will learn to identify and exploit statistical arbitrage opportunities in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79612d",
   "metadata": {},
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "The following libraries are required for this notebook. If you haven't installed them yet, you can do so using running the cell below or by using pip install in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U numpy pandas pyarrow gdown gymnasium stable-baselines3 torch matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664e80a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration section sets up the parameters for the reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DATA\": {\n",
    "        \"drive_folder_id\": \"1uXEBUyySypdsW_ZqL-RZ3d1bWdIZisij\", # google drive folder ID (can be found in the URL)\n",
    "        \"structure\": {\"1d\":\"ohlcv_1d\", # subfolder structure inside the drive folder\n",
    "                      \"1h\":\"ohlcv_1h\",\n",
    "                      \"15m\":\"ohlcv_15m\",\n",
    "                      \"5m\":\"ohlcv_5m\",\n",
    "                      \"1m\":\"ohlcv_1m\"},\n",
    "        \"file_pattern\": \"{TICKER}_{FREQ}.parquet\", # file naming pattern\n",
    "        \"tickers\": [\"BTC\",\"ETH\"], # pairs of assets to trade\n",
    "        \"sampling\": \"1h\", # data sampling frequency (1m, 5m, 15m, 1h, 1d)\n",
    "        \"price_point\": \"close\", # what price point to use for returns calculation (open, high, low, close, etc)\n",
    "        \"forward_fill\": True, # forward fill missing data\n",
    "        \"drop_na_after_ffill\": True, # drop rows with NA values after forward filling\n",
    "        \"cache_dir\": \"./data_cache\", # local cache directory to store downloaded files\n",
    "        \"file_ids\": {\"BTC_1h\": \"1-sBNQpEFGEpVO3GDFCkSZiV3Iaqp2vB_\", # map and define files to download (all data will be downloaded if left empty)\n",
    "                     \"ETH_1h\": \"1kj8G1scpFuEYTTXKEUzF9pwgGI2WFFL9\"\n",
    "                     }\n",
    "    },\n",
    "    \"COINTEGRATION\": {\n",
    "        \"drive_folder_id\": \"1V55Wx2MT-g6rQEo-ZEJIOSA-65zxLJYw\", # google drive folder for cointegration files\n",
    "        \"file_pattern\": \"{ASSET1}_{ASSET2}_window_cointegration.csv\", # cointegration file pattern\n",
    "        \"timestamp_format\": \"%Y-%m-%d %H:%M:%S\", # format of timestamps in cointegration files\n",
    "        \"training_window_days\": 2, # number of days to use for training after cointegration period\n",
    "        \"cache_dir\": \"./cointegration_cache\" # cache directory for cointegration files\n",
    "    },\n",
    "    \"ENV\": {\n",
    "        \"include_cash\": True, # include cash as a third asset\n",
    "        \"shorting\": True, # allow shorting\n",
    "        \"lookback_window\": 64, # lookback for feature slicing, meaning that the observation for each step will include data from the last 'lookback_window' time steps\n",
    "        \"features\": {\n",
    "            \"vol_window\": 64, # rolling volatility window\n",
    "            \"rsi_period\": 14, # RSI lookback period\n",
    "            \"volume_change\": True, # include volume change as a feature\n",
    "            \"normalize\": True # normalize features using rolling z-score\n",
    "        },\n",
    "        \"transaction_costs\": { # transaction costs settings for exchange (e.g. Hyperliquid)\n",
    "            \"commission_bps\": 5.0, # commission in basis points (bps)\n",
    "            \"slippage_bps\": 5.0, # slippage in basis points (bps)\n",
    "        },\n",
    "        \"reward\": {\n",
    "            \"risk_lambda\":0.001 # risk penalty coefficient (lambda), a.k.a. risk aversion factor\n",
    "        },\n",
    "        \"leverage\": {\n",
    "            \"long_cap\": 2.0,  # maximum leverage for long positions\n",
    "            \"short_cap\": 2.0, # maximum leverage for short positions\n",
    "            \"use_asymmetric\": True, # whether to allow different caps for long and short positions\n",
    "        },\n",
    "        \"constraints\": {\n",
    "            \"min_weight\": -2.0, # minimum weight for each asset (for shorting)\n",
    "            \"max_weight\": 2.0,  # maximum weight for each asset (for leverage)\n",
    "            \"sum_to_one\": False # whether weights must sum to one (need to be False for leverage)\n",
    "        },\n",
    "        \"seed\": 42 # random seed for reproducibility\n",
    "    },\n",
    "    \"SPLITS\": { # date splits for training, validation, and testing\n",
    "        \"data_start\":\"2024-09-02\", # start date of the entire dataset\n",
    "        \"data_end\":\"2025-09-02\", # end date of the entire dataset\n",
    "        \"train\":[\"2024-09-02\",\"2025-06-15\"], # training period\n",
    "        \"val\":[\"2025-06-16\",\"2025-07-15\"], # validation period\n",
    "        \"test\":[\"2025-07-16\",\"2025-09-02\"], # testing period\n",
    "        \"walk_forward\": True, # whether to use walk-forward splits (e.g. sliding window)\n",
    "        \"wf_train_span_days\": 180, # training window span in days for walk-forward\n",
    "        \"wf_test_span_days\": 30, # testing window span in days for walk-forward\n",
    "        \"wf_step_days\": 30 # step size in days to move the window forward\n",
    "    },\n",
    "    \"RL\": {\n",
    "        \"timesteps\":10,\n",
    "        \"policy\":\"MlpPolicy\",\n",
    "        \"gamma\":0.99,\n",
    "        \"gae_lambda\":0.95,\n",
    "        \"clip_range\":0.2,\n",
    "        \"n_steps\":1024,\n",
    "        \"batch_size\":256,\n",
    "        \"learning_rate\":3e-4,\n",
    "        \"ent_coef\":0.0,\n",
    "        \"vf_coef\":0.5,\n",
    "        \"max_grad_norm\":0.5\n",
    "    },\n",
    "    \"EVAL\": {\n",
    "        \"plots\":True,\n",
    "        \"reports_dir\":\"./reports\"\n",
    "    },\n",
    "    \"IO\": {\n",
    "        \"models_dir\":\"./models\",\n",
    "        \"tb_logdir\":\"./tb\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568820c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gdown\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import pytz\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Deep Learning\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ee23",
   "metadata": {},
   "source": [
    "## Set Computation Device\n",
    "\n",
    "This section sets the computation device for training the model. It checks if a GPU is available and sets it as the device; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# run on Apple Silicon\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple Silicon GPU.\")\n",
    "\n",
    "# run on CPU (slow)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS are not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981a61",
   "metadata": {},
   "source": [
    "## Set seeds for reproducibility\n",
    "\n",
    "This section sets the random seeds for various libraries to ensure that the results are reproducible.\n",
    "\n",
    "Note: It is good practice to type (set data types) for function and method parameters for better code maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed) # seed for random module\n",
    "    np.random.seed(seed) # seed for numpy module\n",
    "    try:\n",
    "        torch.manual_seed(seed) # seed for torch module\n",
    "        if torch.cuda.is_available(): # seed for CUDA device\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        elif torch.backends.mps.is_available(): # seed for Apple Silicon device\n",
    "            torch.backends.mps.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"]) # set all seeds for reproducibility\n",
    "\n",
    "# set annualization factors for different timeframes\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\n",
    "                 \"5m\":365*24*12,\n",
    "                 \"15m\":365*24*4,\n",
    "                 \"1h\":365*24,\n",
    "                 \"1d\":365}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449a3db",
   "metadata": {},
   "source": [
    "## Helper Functions for Downloading Files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory if it doesn't already exist\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# download entire folder from Google Drive with retry logic\n",
    "def download_drive_folder(root_id: str, out_dir: str, max_retries: int = 3):\n",
    "    print(\"Mirroring Google Drive folder locally...\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            gdown.download_folder(\n",
    "                id=root_id, \n",
    "                output=out_dir, \n",
    "                quiet=False, \n",
    "                use_cookies=False,\n",
    "                remaining_ok=True  # Continue even if some files fail\n",
    "            )\n",
    "            print(\"Folder mirroring complete.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Download attempt {attempt + 1} failed. Retrying... Error: {str(e)}\")\n",
    "                time.sleep(5)  # Wait 5 seconds before retrying\n",
    "            else:\n",
    "                print(f\"All download attempts failed for folder {root_id}. Error: {str(e)}\")\n",
    "                return False\n",
    "\n",
    "# download specific files from Google Drive by their file IDs with retry logic\n",
    "def targeted_download_by_ids(file_id_map: Dict[str, str], out_dir: str, max_retries: int = 3):\n",
    "    ensure_dir(out_dir)\n",
    "    \n",
    "    for name, fid in file_id_map.items():\n",
    "        # check if file already exists in local cache folder\n",
    "        if os.path.exists(os.path.join(out_dir, name)) or os.path.exists(os.path.join(out_dir, f\"{name}.parquet\")):\n",
    "            print(f\"File {name} already exists in cache. Skipping download.\")\n",
    "            continue\n",
    "        \n",
    "        suffix = name if name.endswith(\".parquet\") else f\"{name}.parquet\" # ensure .parquet suffix\n",
    "        out_path = os.path.join(out_dir, suffix) # full output path\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Downloading {name} -> {out_path}\")\n",
    "                url = f\"https://drive.google.com/uc?id={fid}\"\n",
    "                success = gdown.download(url, out_path, quiet=False, use_cookies=False, verify=False)\n",
    "                if success:\n",
    "                    break\n",
    "                else:\n",
    "                    raise Exception(\"Download failed\")\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Download attempt {attempt + 1} failed for {name}. Retrying... Error: {str(e)}\")\n",
    "                    time.sleep(5)  # Wait 5 seconds before retrying\n",
    "                else:\n",
    "                    print(f\"All download attempts failed for {name}. Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151dd54",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Once the data is downloaded, this section loads the data into a pandas DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download cointegration data\n",
    "COINT_ROOT_ID = CONFIG[\"COINTEGRATION\"][\"drive_folder_id\"]\n",
    "COINT_CACHE_DIR = CONFIG[\"COINTEGRATION\"][\"cache_dir\"]\n",
    "ensure_dir(COINT_CACHE_DIR)\n",
    "\n",
    "def should_process_file(filename: str) -> bool:\n",
    "    \"\"\"Determine if a file should be processed based on filename.\"\"\"\n",
    "    excluded_files = ['historical_pairs_with_spreads.csv']\n",
    "    return not any(excluded in filename for excluded in excluded_files)\n",
    "\n",
    "# Download cointegration folder with improved error handling\n",
    "print(\"Downloading cointegration data...\")\n",
    "success = download_drive_folder(COINT_ROOT_ID, COINT_CACHE_DIR)\n",
    "\n",
    "if not success:\n",
    "    print(\"Warning: Some cointegration files may be missing. Continuing with available data.\")\n",
    "\n",
    "def get_all_assets_from_cointegration():\n",
    "    \"\"\"Scan cointegration files to get all unique assets.\"\"\"\n",
    "    assets = set()\n",
    "    pattern = os.path.join(COINT_CACHE_DIR, \"**\", \"*_window_cointegration.csv\")\n",
    "    coint_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    for file_path in coint_files:\n",
    "        if not should_process_file(os.path.basename(file_path)):\n",
    "            continue\n",
    "        base_name = os.path.basename(file_path)\n",
    "        pair_assets = base_name.replace(\"_window_cointegration.csv\", \"\").split(\"_\")[:2]\n",
    "        assets.update(pair_assets)\n",
    "    \n",
    "    return sorted(list(assets))\n",
    "\n",
    "def load_cointegration_data():\n",
    "    \"\"\"Load all cointegration test results and return a DataFrame with training windows.\"\"\"\n",
    "    all_periods = []\n",
    "    \n",
    "    # Find all cointegration CSV files\n",
    "    pattern = os.path.join(COINT_CACHE_DIR, \"**\", \"*_window_cointegration.csv\")\n",
    "    coint_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    if not coint_files:\n",
    "        print(\"Warning: No cointegration files found in cache directory.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for file_path in coint_files:\n",
    "        if not should_process_file(os.path.basename(file_path)):\n",
    "            print(f\"Skipping excluded file: {os.path.basename(file_path)}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Extract asset names from filename\n",
    "            base_name = os.path.basename(file_path)\n",
    "            assets = base_name.replace(\"_window_cointegration.csv\", \"\").split(\"_\")[:2]\n",
    "            \n",
    "            # Read cointegration results\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['start'] = pd.to_datetime(df['start'], format=CONFIG[\"COINTEGRATION\"][\"timestamp_format\"])\n",
    "            df['end'] = pd.to_datetime(df['end'], format=CONFIG[\"COINTEGRATION\"][\"timestamp_format\"])\n",
    "            \n",
    "            # Filter for cointegrated periods\n",
    "            coint_periods = df[df['cointegrated']]\n",
    "            \n",
    "            for _, row in coint_periods.iterrows():\n",
    "                # Calculate training period (2 days after cointegration period)\n",
    "                train_start = row['end']\n",
    "                train_end = train_start + pd.Timedelta(days=CONFIG[\"COINTEGRATION\"][\"training_window_days\"])\n",
    "                \n",
    "                all_periods.append({\n",
    "                    'asset1': assets[0],\n",
    "                    'asset2': assets[1],\n",
    "                    'coint_start': row['start'],\n",
    "                    'coint_end': row['end'],\n",
    "                    'train_start': train_start,\n",
    "                    'train_end': train_end,\n",
    "                    'alpha': row['alpha'],\n",
    "                    'beta': row['beta'],\n",
    "                    'adf_p': row['adf_p'],\n",
    "                    'correlation': row['correlation']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(all_periods)\n",
    "\n",
    "# Get all unique assets from cointegration files\n",
    "all_assets = get_all_assets_from_cointegration()\n",
    "print(f\"Found {len(all_assets)} unique assets in cointegration files: {all_assets}\")\n",
    "\n",
    "# Update CONFIG with discovered assets\n",
    "CONFIG[\"DATA\"][\"tickers\"] = all_assets\n",
    "\n",
    "# Load cointegration data\n",
    "coint_df = load_cointegration_data()\n",
    "print(f\"Loaded {len(coint_df)} cointegration periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537b4fd",
   "metadata": {},
   "source": [
    "## Fetch Data from Google Drive\n",
    "\n",
    "This section handles downloading data from Google Drive. It supports two methods: downloading an entire folder or downloading specific files by their IDs. The data will be cached locally for faster subsequent loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_ID = CONFIG[\"DATA\"][\"drive_folder_id\"] # suffix for Google Drive download URL\n",
    "CACHE_DIR = CONFIG[\"DATA\"][\"cache_dir\"] # data will be cached here for faster subsequent loading\n",
    "FILE_IDS = CONFIG[\"DATA\"][\"file_ids\"] # mapping of data to its URL suffix\n",
    "\n",
    "\n",
    "# Get configuration parameters\n",
    "sampling = CONFIG[\"DATA\"][\"sampling\"]\n",
    "subfolder = CONFIG[\"DATA\"][\"structure\"][sampling]\n",
    "pattern_fmt = CONFIG[\"DATA\"][\"file_pattern\"]\n",
    "forward_fill = CONFIG[\"DATA\"][\"forward_fill\"]\n",
    "drop_na_after_ffill = CONFIG[\"DATA\"][\"drop_na_after_ffill\"]\n",
    "\n",
    "# function to find parquet file path\n",
    "def find_parquet_path(ticker: str, sampling: str) -> str:\n",
    "    fname = pattern_fmt.format(TICKER=ticker, FREQ=sampling)\n",
    "    # try subfolder first\n",
    "    candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", subfolder, fname), recursive=True)\n",
    "    # if not found, try flat cache\n",
    "    if not candidates:\n",
    "        candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", fname), recursive=True)\n",
    "    # if still not found, try direct file in cache (for file_ids downloads)\n",
    "    if not candidates:\n",
    "        direct_path = os.path.join(CACHE_DIR, fname)\n",
    "        if os.path.exists(direct_path):\n",
    "            candidates = [direct_path]\n",
    "    # if nothing is found at all, raise an error\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {fname} under {CACHE_DIR}.\")\n",
    "    return candidates[0]\n",
    "\n",
    "# function to localize timestamps and align dataframes\n",
    "def localize_and_align(df: pd.DataFrame, tz_in: str = None, tz_out: str = None) -> pd.DataFrame:\n",
    "    # convert millisecond timestamps to datetime\n",
    "    if 'datetime' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['datetime'], unit='ms', utc=True)\n",
    "        df = df.set_index('timestamp')\n",
    "    # make column names lowercase for consistency\n",
    "    cols = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    return df.sort_index()\n",
    "\n",
    "# Load data for all assets found in cointegration files\n",
    "print(f\"Loading price data for {len(all_assets)} assets...\")\n",
    "dfs = {}\n",
    "missing_assets = []\n",
    "for t in all_assets:\n",
    "    try:\n",
    "        print(f\"Loading {t}...\")\n",
    "        pth = find_parquet_path(t, sampling)\n",
    "        tmp = pd.read_parquet(pth)\n",
    "        tmp = localize_and_align(tmp)\n",
    "        if forward_fill:\n",
    "            tmp = tmp.ffill()\n",
    "        if drop_na_after_ffill:\n",
    "            tmp = tmp.dropna()\n",
    "        dfs[t] = tmp\n",
    "        print(f\"Loaded {t} data with shape {tmp.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {t}: {str(e)}\")\n",
    "        missing_assets.append(t)\n",
    "\n",
    "if missing_assets:\n",
    "    print(f\"\\nWarning: Could not load data for {len(missing_assets)} assets: {missing_assets}\")\n",
    "    # Remove missing assets from the CONFIG and all_assets\n",
    "    all_assets = [a for a in all_assets if a not in missing_assets]\n",
    "    CONFIG[\"DATA\"][\"tickers\"] = all_assets\n",
    "    print(f\"Continuing with {len(all_assets)} available assets\")\n",
    "\n",
    "# Find common time index across all loaded assets\n",
    "print(\"\\nAligning time indices...\")\n",
    "common_index = None\n",
    "for t, df in dfs.items():\n",
    "    if common_index is None:\n",
    "        common_index = df.index\n",
    "    else:\n",
    "        common_index = common_index.intersection(df.index)\n",
    "\n",
    "# Reindex all dataframes to the common index and drop any remaining NA values\n",
    "print(\"Reindexing to common timeframe...\")\n",
    "for t in all_assets:\n",
    "    if t in dfs:  # Only process successfully loaded assets\n",
    "        dfs[t] = dfs[t].reindex(common_index).dropna()\n",
    "\n",
    "# Print final shapes to verify alignment\n",
    "print(\"\\nFinal dataset shapes:\")\n",
    "shapes = {t: dfs[t].shape for t in dfs.keys()}\n",
    "for asset, shape in shapes.items():\n",
    "    print(f\"{asset}: {shape}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No price data could be loaded for any assets!\")\n",
    "\n",
    "print(\"\\nTime range of data:\")\n",
    "print(f\"Start: {common_index[0]}\")\n",
    "print(f\"End: {common_index[-1]}\")\n",
    "print(f\"Total timepoints: {len(common_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_all = True if FILE_IDS == {} else False # this determines whether to download entire folder or specific files by their IDs (s. CONFIG)\n",
    "\n",
    "# check if cache directory exists, if not create it\n",
    "ensure_dir(CACHE_DIR)\n",
    "\n",
    "if download_all: # download entire folder\n",
    "    download_drive_folder(ROOT_ID, CACHE_DIR)\n",
    "else: # download specific files by their IDs\n",
    "    targeted_download_by_ids(FILE_IDS, CACHE_DIR)\n",
    "\n",
    "print(\"Download step complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling = CONFIG[\"DATA\"][\"sampling\"] # data sampling frequency (1m, 5m, 15m, 1h, 1d)\n",
    "subfolder = CONFIG[\"DATA\"][\"structure\"][sampling] # subfolder name inside the drive folder is determined by the sampling frequency\n",
    "pattern_fmt = CONFIG[\"DATA\"][\"file_pattern\"] # file naming pattern (s. CONFIG)\n",
    "tickers = CONFIG[\"DATA\"][\"tickers\"] # list of tickers to load\n",
    "forward_fill = CONFIG[\"DATA\"][\"forward_fill\"] # whether to forward fill missing data\n",
    "drop_na_after_ffill = CONFIG[\"DATA\"][\"drop_na_after_ffill\"] # whether to drop NA values after forward filling\n",
    "\n",
    "# function to find parquet file path\n",
    "def find_parquet_path(ticker: str, sampling: str) -> str:\n",
    "    fname = pattern_fmt.format(TICKER=ticker, FREQ=sampling)\n",
    "    # try subfolder first\n",
    "    candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", subfolder, fname), recursive=True)\n",
    "    # if not found, try flat cache\n",
    "    if not candidates:\n",
    "        candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", fname), recursive=True)\n",
    "    # if still not found, try direct file in cache (for file_ids downloads)\n",
    "    if not candidates:\n",
    "        direct_path = os.path.join(CACHE_DIR, fname)\n",
    "        if os.path.exists(direct_path):\n",
    "            candidates = [direct_path]\n",
    "    # if nothing is found at all, raise an error\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {fname} under {CACHE_DIR}.\")\n",
    "    return candidates[0]\n",
    "\n",
    "# function to localize timestamps and align dataframes\n",
    "def localize_and_align(df: pd.DataFrame, tz_in: str = None, tz_out: str = None) -> pd.DataFrame:\n",
    "    # convert millisecond timestamps to datetime\n",
    "    if 'datetime' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['datetime'], unit='ms', utc=True)\n",
    "        df = df.set_index('timestamp')\n",
    "    # make column names lowercase for consistency\n",
    "    cols = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    return df.sort_index()\n",
    "\n",
    "dfs = {}\n",
    "for t in tickers:\n",
    "    pth = find_parquet_path(t, sampling) # find file path\n",
    "    tmp = pd.read_parquet(pth) # read parquet file\n",
    "    tmp = localize_and_align(tmp) # standardize timestamps and column names\n",
    "    if forward_fill: # forward fill missing data if specified\n",
    "        tmp = tmp.ffill()\n",
    "    if drop_na_after_ffill: # drop rows with NA values after forward filling\n",
    "        tmp = tmp.dropna()\n",
    "    dfs[t] = tmp # store in dictionary\n",
    "\n",
    "common_index = None\n",
    "# find common index across all dataframes\n",
    "for t, df in dfs.items():\n",
    "    common_index = df.index if common_index is None else common_index.intersection(df.index)\n",
    "# reindex all dataframes to the common index and drop any remaining NA values\n",
    "for t in tickers:\n",
    "    dfs[t] = dfs[t].reindex(common_index).dropna()\n",
    "\n",
    "# print the shape of each dataframe to ensure alignment\n",
    "print({t: dfs[t].shape for t in tickers})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a77e6",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "This section performs feature engineering on the loaded data. It includes creating new features, normalizing data, and preparing the dataset for training the reinforcement learning agent. To ensure statistical arbitrage strategy is imitated, create relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cfg = CONFIG[\"ENV\"][\"features\"]\n",
    "price_col = CONFIG[\"DATA\"][\"price_point\"]\n",
    "\n",
    "# function to compute relative strength index (momentum indicator)\n",
    "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff() # compute price changes\n",
    "    up = (delta.clip(lower=0)).ewm(alpha=1/period, adjust=False).mean() # average of upward price changes\n",
    "    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean() # average of downward price changes\n",
    "    rs = up / (down + 1e-12) # relative strength ratio indicates if upward or downward momentum is stronger\n",
    "    rsi = 100 - (100 / (1 + rs)) # converts RS to RSI bounded between 0 and 100\n",
    "    return rsi\n",
    "\n",
    "# function to create features dataframe\n",
    "def make_features(df: pd.DataFrame, price_col: str, vol_window: int, rsi_period: int, volume_change: bool):\n",
    "    out = pd.DataFrame(index=df.index) # initialize output dataframe\n",
    "    out[\"ret\"] = np.log(df[price_col]).diff(1) # log returns\n",
    "    out[\"vol\"] = out[\"ret\"].rolling(vol_window).std().fillna(0.0) # rolling volatility\n",
    "    out[\"rsi\"] = compute_rsi(df[price_col], rsi_period).fillna(50.0) # relative strength index\n",
    "    if \"volume\" in df.columns and volume_change: # log volume change if volume data is available\n",
    "        out[\"volchg\"] = np.log(df[\"volume\"].replace(0, np.nan)).diff().fillna(0.0) # log volume change\n",
    "    else:\n",
    "        out[\"volchg\"] = 0.0 # if no volume data, set to zero\n",
    "    return out\n",
    "\n",
    "features_by_ticker = {}\n",
    "for t in tickers: # create features for each ticker\n",
    "    fdf = make_features(dfs[t], price_col, feat_cfg[\"vol_window\"], feat_cfg[\"rsi_period\"], feat_cfg[\"volume_change\"])\n",
    "    features_by_ticker[t] = fdf # store features in dictionary\n",
    "\n",
    "panel_cols = []\n",
    "for t in tickers: # create multi-index columns\n",
    "    for col in [\"ret\",\"vol\",\"rsi\",\"volchg\"]:\n",
    "        panel_cols.append((t, col))\n",
    "panel = pd.concat([features_by_ticker[t][[\"ret\",\"vol\",\"rsi\",\"volchg\"]] for t in tickers], axis=1) # combine features horizontally\n",
    "panel.columns = pd.MultiIndex.from_tuples(panel_cols, names=[\"ticker\",\"feature\"]) # set multi-index columns\n",
    "panel = panel.dropna() # remove rows with missing values\n",
    "\n",
    "print(panel.tail())\n",
    "panel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1a418",
   "metadata": {},
   "source": [
    "## Feature scaling and state tensor construction\n",
    "\n",
    "This section normalizes the features and constructs the state tensors required for training the reinforcement learning agent. State tensors are multi-dimensional arrays that represent the current state of the environment that the RL agent uses to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = CONFIG[\"ENV\"][\"lookback_window\"] # lookback window \n",
    "normalize = CONFIG[\"ENV\"][\"features\"][\"normalize\"] # normalization method\n",
    "\n",
    "# rolling z-score normalization for features\n",
    "def rolling_zscore(df: pd.DataFrame, window: int = 256) -> pd.DataFrame:\n",
    "    mu = df.rolling(window).mean() # rolling mean\n",
    "    sigma = df.rolling(window).std().replace(0, np.nan) # rolling std dev, replace 0 with NaN to avoid division by zero\n",
    "    z = (df - mu) / (sigma + 1e-12) # z-score normalization\n",
    "    return z.fillna(0.0) # fill NaNs with 0.0 and return z-scored dataframe\n",
    "\n",
    "# build state tensor\n",
    "def build_state_tensor(panel: pd.DataFrame, lookback: int, normalize: bool = False):\n",
    "    # normalization step for each feature\n",
    "    if normalize: # group by feature and apply z-score while preserving MultiIndex\n",
    "        normalized = pd.DataFrame(index=panel.index) # empty dataframe with same index as features dataframes to hold scaled features\n",
    "        for feature in panel.columns.unique(level=1): # iterate over features\n",
    "            feature_data = panel.xs(feature, level=1, axis=1)\n",
    "            z_scored = rolling_zscore(feature_data, window=max(lookback*2, 256)) # take double the lookback value for rolling\n",
    "            for ticker in z_scored.columns: # reconstruct MultiIndex columns\n",
    "                normalized[(ticker, feature)] = z_scored[ticker]\n",
    "        normalized.columns = pd.MultiIndex.from_tuples(normalized.columns, names=[\"ticker\", \"feature\"]) # set MultiIndex columns\n",
    "    else: # no normalization, use raw features\n",
    "        normalized = panel.copy()\n",
    "\n",
    "    # organize data by tickers, features, and time\n",
    "    tickers = sorted(panel.columns.unique(level=0)) # ensure consistent order\n",
    "    features = sorted(panel.columns.unique(level=1)) # feature order\n",
    "    times = normalized.index # time index\n",
    "\n",
    "    # create sliding windows of lookback length\n",
    "    # extract feature data for the window and stack into a 3D tensor (tickers x features x lookback)\n",
    "    X, y_ret, inst_vol = [], [], [] # lists to hold state tensors, next returns, and current volatilities\n",
    "    for i in range(lookback, len(times)-1): # iterate over time index in lookback window to have a sliding window with each time point as a step\n",
    "        window_slice = normalized.iloc[i-lookback:i]\n",
    "        frames = []\n",
    "        for t in tickers: # save the windows of each ticker\n",
    "            frames.append(window_slice[t].T.values)\n",
    "        tensor = np.stack(frames, axis=0) # stack into 3D tensor\n",
    "        X.append(tensor) # append tensor to list\n",
    "        nxt = panel.iloc[i+1] # next period returns\n",
    "        y_ret.append(np.array([nxt[(t, \"ret\")] for t in tickers], dtype=float)) # save to list\n",
    "        cur = panel.iloc[i] # current period volatilities\n",
    "        inst_vol.append(np.array([cur[(t, \"vol\")] for t in tickers], dtype=float)) # save to list\n",
    "\n",
    "    X = np.array(X, dtype=np.float32) # convert list of state tensors of tensors to 3D numpy array\n",
    "    y_ret = np.array(y_ret, dtype=np.float32) # convert list of next returns to 2D numpy array\n",
    "    inst_vol = np.array(inst_vol, dtype=np.float32) # convert list of volatilities to 2D numpy array\n",
    "    return X, y_ret, inst_vol, tickers, features, times[lookback+1:]\n",
    "\n",
    "X_all, R_all, VOL_all, TICKER_ORDER, FEAT_ORDER, TIME_INDEX = build_state_tensor(\n",
    "    panel, lookback=lookback, normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"State tensor:\", X_all.shape, \"Returns:\", R_all.shape, \"InstVol:\", VOL_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f60af",
   "metadata": {},
   "source": [
    "## Define Splits and Adjust Timezones\n",
    "\n",
    "This section defines the training and validation splits for the dataset. It ensures that the data is divided appropriately to train the model, validate its performance during training, and test its final performance on unseen data. Also, it adjusts the timezones of the datetime indices to ensure consistency across the dataset. This is necessary in case the data comes from multiple sources with different timezone settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create boolean mask for date slicing\n",
    "def date_slice_mask(times: pd.DatetimeIndex, start: str, end: str):\n",
    "    # convert input dates to UTC timestamps\n",
    "    start_ts = pd.Timestamp(start).tz_localize('UTC')\n",
    "    end_ts = pd.Timestamp(end).tz_localize('UTC')\n",
    "    \n",
    "    # ensure time index is in UTC\n",
    "    if times.tz is None: # if naive, localize to UTC\n",
    "        times = times.tz_localize('UTC')\n",
    "    elif times.tz != pytz.UTC: # if timezone-aware but not UTC, convert to UTC\n",
    "        times = times.tz_convert('UTC')\n",
    "        \n",
    "    return (times >= start_ts) & (times <= end_ts) # return boolean mask for splits later on\n",
    "\n",
    "def create_cointegration_splits(coint_df: pd.DataFrame, price_data_index: pd.DatetimeIndex):\n",
    "    \"\"\"Create training windows based on cointegration periods.\"\"\"\n",
    "    train_masks = []\n",
    "    val_masks = []\n",
    "    test_masks = []\n",
    "    \n",
    "    # Ensure indices are timezone-aware\n",
    "    if price_data_index.tz is None:\n",
    "        price_data_index = price_data_index.tz_localize('UTC')\n",
    "    \n",
    "    # Sort periods chronologically\n",
    "    coint_df = coint_df.sort_values('train_start')\n",
    "    \n",
    "    # Split cointegration periods into train/val/test\n",
    "    split_point1 = int(0.7 * len(coint_df))  # 70% for training\n",
    "    split_point2 = int(0.85 * len(coint_df))  # 15% for validation\n",
    "    \n",
    "    train_periods = coint_df.iloc[:split_point1]\n",
    "    val_periods = coint_df.iloc[split_point1:split_point2]\n",
    "    test_periods = coint_df.iloc[split_point2:]\n",
    "    \n",
    "    # Create masks for each period type\n",
    "    for periods, masks in [(train_periods, train_masks), \n",
    "                          (val_periods, val_masks), \n",
    "                          (test_periods, test_masks)]:\n",
    "        for _, row in periods.iterrows():\n",
    "            mask = (price_data_index >= row['train_start']) & (price_data_index <= row['train_end'])\n",
    "            masks.append(mask)\n",
    "    \n",
    "    # Combine masks for each split with OR operation\n",
    "    train_mask = np.logical_or.reduce(train_masks) if train_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    val_mask = np.logical_or.reduce(val_masks) if val_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    test_mask = np.logical_or.reduce(test_masks) if test_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    \n",
    "    print(f\"Training windows: {train_mask.sum()} timesteps\")\n",
    "    print(f\"Validation windows: {val_mask.sum()} timesteps\")\n",
    "    print(f\"Testing windows: {test_mask.sum()} timesteps\")\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "# Create training masks based on cointegration periods\n",
    "train_mask, val_mask, test_mask = create_cointegration_splits(coint_df, TIME_INDEX)\n",
    "\n",
    "# Create splits for training\n",
    "SPLITS = [{\n",
    "    \"name\": \"CointegrationSplit\",\n",
    "    \"train\": train_mask,\n",
    "    \"val\": val_mask,\n",
    "    \"test\": test_mask\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be710c8e",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "This is a custom Gymnasium environment for portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioWeightsEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env):\n",
    "        super().__init__()\n",
    "        self.X = X # state tensor\n",
    "        self.R = R # next period returns\n",
    "        self.VOL = VOL # current period volatilities\n",
    "        self.tickers = tickers # list of assets\n",
    "        self.lookback = lookback # lookback window\n",
    "        self.cfg = cfg_env # environment configuration taken from CONFIG\n",
    "\n",
    "        self.n_assets = len(tickers) # number of assets\n",
    "        self.include_cash = cfg_env[\"include_cash\"] # whether to include cash as an asset\n",
    "        self.shorting = cfg_env[\"shorting\"] # whether shorting is allowed\n",
    "        self.dim_action = self.n_assets + (1 if self.include_cash else 0) # action space dimension: one weight per asset (+1 for cash if included)\n",
    "\n",
    "        obs_dim = self.n_assets * self.X.shape[2] * self.lookback # observation space dimension: tickers x features x lookback\n",
    "        self.observation_space = spaces.Box(low=-5, high=5, shape=(obs_dim,), dtype=np.float32) # needed for Gym to define feature bounds (±5 for z-scored features)\n",
    "        \n",
    "        # Update action space to allow for leveraged positions\n",
    "        if self.shorting:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=-cfg_env[\"leverage\"][\"short_cap\"],\n",
    "                high=cfg_env[\"leverage\"][\"long_cap\"],\n",
    "                shape=(self.dim_action,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=0.0,\n",
    "                high=cfg_env[\"leverage\"][\"long_cap\"],\n",
    "                shape=(self.dim_action,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4 # commission in decimal form\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4 # slippage in decimal form\n",
    "        self.risk_lambda = cfg_env[\"reward\"][\"risk_lambda\"] # defines risk aversion in reward function\n",
    "\n",
    "        # Leverage settings\n",
    "        self.long_cap = cfg_env[\"leverage\"][\"long_cap\"]\n",
    "        self.short_cap = cfg_env[\"leverage\"][\"short_cap\"]\n",
    "        self.use_asymmetric = cfg_env[\"leverage\"][\"use_asymmetric\"]\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42))\n",
    "\n",
    "    def _to_obs(self, t):\n",
    "        # Get the current window of observations\n",
    "        arr = self.X[t].reshape(-1).astype(np.float32)\n",
    "        return arr\n",
    "    \n",
    "    # project raw actions to asset weights with leverage\n",
    "    def _project_weights(self, a):\n",
    "        if self.use_asymmetric:\n",
    "            # Apply asymmetric leverage caps\n",
    "            w = np.clip(a, -self.short_cap if self.shorting else 0.0, self.long_cap)\n",
    "        else:\n",
    "            # Use symmetric leverage cap\n",
    "            max_leverage = max(self.long_cap, self.short_cap)\n",
    "            w = np.clip(a, -max_leverage if self.shorting else 0.0, max_leverage)\n",
    "        return w\n",
    "\n",
    "    # reset environment to initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0 # reset time index\n",
    "        self.portfolio_value = 1.0 # reset portfolio value\n",
    "        self.w = np.zeros(self.dim_action) # initialize with zero weights\n",
    "        if self.include_cash:\n",
    "            self.w[-1] = 1.0  # start with all cash\n",
    "        obs = self._to_obs(self.t)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        w_target = self._project_weights(action) # get new target weights from raw actions\n",
    "        turnover = np.sum(np.abs(w_target - self.w)) # calculate turnover as sum of absolute weight changes\n",
    "        trading_cost = (self.commission + self.slippage) * turnover # total trading cost based on turnover\n",
    "\n",
    "        asset_w_prev = self.w[:self.n_assets] # previous weights excluding cash\n",
    "        asset_ret = np.dot(asset_w_prev, self.R[self.t]) # calculate asset return based on previous weights and next period returns\n",
    "        inst_vol = np.dot(asset_w_prev, self.VOL[self.t]) # calculate instantaneous volatility based on previous weights and current volatilities\n",
    "\n",
    "        # reward function: asset return minus trading costs and risk penalty\n",
    "        reward = asset_ret - trading_cost - self.risk_lambda * inst_vol\n",
    "\n",
    "        self.portfolio_value *= math.exp(asset_ret - trading_cost) # update portfolio value using log returns\n",
    "\n",
    "        self.w = w_target # update calculated weights for next step\n",
    "        self.t += 1 # move to next time step\n",
    "        terminated = (self.t >= len(self.R)-1) # episode ends if we reach the end of the data\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        obs = np.clip(obs, -5.0, 5.0) # clip observations to previously defined bounds (±5 for z-scored features)\n",
    "        \n",
    "        # Additional info for monitoring leveraged positions\n",
    "        leverage = np.sum(np.abs(self.w[:self.n_assets]))\n",
    "        info = {\n",
    "            \"portfolio_value\": self.portfolio_value, \n",
    "            \"turnover\": turnover, \n",
    "            \"inst_vol\": inst_vol, \n",
    "            \"asset_ret\": asset_ret,\n",
    "            \"total_leverage\": leverage\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_mask(X, R, VOL, mask: np.ndarray):\n",
    "    idx = np.where(mask)[0] # get all indices where mask is True\n",
    "    \n",
    "    # If we have no valid indices, return empty arrays with correct shapes\n",
    "    if len(idx) == 0:\n",
    "        empty_shape_x = list(X.shape)\n",
    "        empty_shape_x[0] = 0\n",
    "        empty_shape_r = list(R.shape)\n",
    "        empty_shape_r[0] = 0\n",
    "        empty_shape_v = list(VOL.shape)\n",
    "        empty_shape_v[0] = 0\n",
    "        return np.zeros(empty_shape_x), np.zeros(empty_shape_r), np.zeros(empty_shape_v)\n",
    "    \n",
    "    return X[idx], R[idx], VOL[idx] # return only the selected slices of data\n",
    "\n",
    "def make_env_from_mask(mask, name=\"env\"):\n",
    "    X_s, R_s, V_s = slice_by_mask(X_all, R_all, VOL_all, mask)\n",
    "    env = PortfolioWeightsEnv(X_s, R_s, V_s, TICKER_ORDER, CONFIG[\"ENV\"][\"lookback_window\"], CONFIG[\"ENV\"])\n",
    "    env = Monitor(env, filename=None)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annualize_factor(sampling: str):\n",
    "    return ANNUALIZATION.get(sampling, 365*24)\n",
    "\n",
    "def compute_metrics(equity_curve: pd.Series, sampling: str, turnover_series: pd.Series = None):\n",
    "    ret = equity_curve.pct_change().dropna()\n",
    "    ann = annualize_factor(sampling)\n",
    "    mu = ret.mean() * ann\n",
    "    sigma = ret.std() * math.sqrt(ann)\n",
    "    sharpe = mu / (sigma + 1e-12)\n",
    "    downside = ret[ret < 0].std() * math.sqrt(ann)\n",
    "    sortino = mu / (downside + 1e-12)\n",
    "    if len(equity_curve) > 1:\n",
    "        # Calculate years based on number of samples and sampling frequency\n",
    "        if isinstance(equity_curve.index, pd.DatetimeIndex):\n",
    "            dt_years = (equity_curve.index[-1] - equity_curve.index[0]).total_seconds() / (365 * 24 * 3600)\n",
    "        else:\n",
    "            # If using RangeIndex, calculate based on sampling frequency\n",
    "            samples = len(equity_curve)\n",
    "            samples_per_year = annualize_factor(sampling)\n",
    "            dt_years = samples / samples_per_year\n",
    "        dt_years = float(dt_years) if float(dt_years) != 0 else 1e-12\n",
    "        cagr = (equity_curve.iloc[-1] / equity_curve.iloc[0]) ** (1/dt_years) - 1\n",
    "    else:\n",
    "        cagr = 0.0\n",
    "    cummax = equity_curve.cummax()\n",
    "    dd = (equity_curve / cummax - 1).min()\n",
    "    maxdd = float(dd)\n",
    "    calmar = mu / (abs(maxdd) + 1e-12)\n",
    "    hit_ratio = (ret > 0).mean()\n",
    "    turnover = turnover_series.mean() if turnover_series is not None and len(turnover_series)>0 else np.nan\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"Sortino\": sortino, \"MaxDrawdown\": maxdd, \"Calmar\": calmar, \"Volatility\": sigma, \"Turnover\": turnover, \"HitRatio\": hit_ratio}\n",
    "\n",
    "def plot_series(series: pd.Series, title: str):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(series.index, series.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_periods(coint_df: pd.DataFrame, price_index: pd.DatetimeIndex):\n",
    "    \"\"\"Plot timeline showing training periods and their overlap.\"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    y = 0\n",
    "    for _, row in coint_df.iterrows():\n",
    "        plt.hlines(y, row['train_start'], row['train_end'], 'blue', alpha=0.3)\n",
    "        y += 1\n",
    "    plt.title(\"Training Periods Timeline\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Period Index\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def backtest_env(env: gym.Env, model=None):\n",
    "    # Get the unwrapped environment\n",
    "    unwrapped = env.unwrapped if hasattr(env, 'unwrapped') else env\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    pv, turns = [], []\n",
    "    leverage = []  # Track leverage over time\n",
    "    \n",
    "    for t in range(len(unwrapped.R)-1):\n",
    "        if model is None:\n",
    "            action = np.ones(unwrapped.dim_action)/unwrapped.dim_action\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        pv.append(info[\"portfolio_value\"])\n",
    "        turns.append(info[\"turnover\"])\n",
    "        leverage.append(info.get(\"total_leverage\", 0))\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    idx = pd.RangeIndex(start=0, stop=len(pv), step=1)\n",
    "    ec = pd.Series(pv, index=idx)\n",
    "    to = pd.Series(turns, index=idx)\n",
    "    lev = pd.Series(leverage, index=idx)\n",
    "    \n",
    "    return ec, to, lev\n",
    "\n",
    "# Plot cointegration training periods\n",
    "print(\"Visualizing training periods timeline...\")\n",
    "plot_training_periods(coint_df, TIME_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(CONFIG[\"IO\"][\"models_dir\"])\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\n=== Training on split: {split['name']} ===\")\n",
    "    train_env = make_env_from_mask(split[\"train\"], name=f\"{split['name']}_train\")\n",
    "    eval_env  = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "\n",
    "    vec_train = DummyVecEnv([lambda: train_env])\n",
    "    vec_eval  = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=CONFIG[\"RL\"][\"policy\"],\n",
    "        env=vec_train,\n",
    "        gamma=CONFIG[\"RL\"][\"gamma\"],\n",
    "        gae_lambda=CONFIG[\"RL\"][\"gae_lambda\"],\n",
    "        clip_range=CONFIG[\"RL\"][\"clip_range\"],\n",
    "        n_steps=CONFIG[\"RL\"][\"n_steps\"],\n",
    "        batch_size=CONFIG[\"RL\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"RL\"][\"learning_rate\"],\n",
    "        ent_coef=CONFIG[\"RL\"][\"ent_coef\"],\n",
    "        vf_coef=CONFIG[\"RL\"][\"vf_coef\"],\n",
    "        max_grad_norm=CONFIG[\"RL\"][\"max_grad_norm\"],\n",
    "        tensorboard_log=CONFIG[\"IO\"][\"tb_logdir\"],\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_eval, \n",
    "        best_model_save_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "        log_path=CONFIG[\"IO\"][\"models_dir\"], \n",
    "        eval_freq=10000,\n",
    "        deterministic=True, \n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=CONFIG[\"RL\"][\"timesteps\"], callback=eval_callback)\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{split['name']}.zip\")\n",
    "    model.save(model_path)\n",
    "    print(\"Saved model:\", model_path)\n",
    "\n",
    "    test_env = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "    ec, to = backtest_env(test_env, model=model)\n",
    "\n",
    "    idx = np.where(split[\"test\"])[0]\n",
    "    R_test = R_all[idx]\n",
    "    ew = np.ones(len(TICKER_ORDER))/len(TICKER_ORDER)\n",
    "    ec_bench = [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        ec_bench.append(ec_bench[-1]*math.exp(np.dot(ew, R_test[i])))\n",
    "    ec_bench = pd.Series(ec_bench, index=ec.index)\n",
    "\n",
    "    bh_btc, bh_eth = [1.0], [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        bh_btc.append(bh_btc[-1]*math.exp(R_test[i][0]))\n",
    "        bh_eth.append(bh_eth[-1]*math.exp(R_test[i][1]))\n",
    "    bh_btc = pd.Series(bh_btc, index=ec.index)\n",
    "    bh_eth = pd.Series(bh_eth, index=ec.index)\n",
    "\n",
    "    m_model = compute_metrics(ec, CONFIG[\"DATA\"][\"sampling\"], to)\n",
    "    m_ew    = compute_metrics(ec_bench, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_btc   = compute_metrics(bh_btc, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_eth   = compute_metrics(bh_eth, CONFIG[\"DATA\"][\"sampling\"])\n",
    "\n",
    "    RESULTS.append({\"split\": split[\"name\"], \"model\": m_model, \"equal_weight\": m_ew, \"buy_and_hold_BTC\": m_btc, \"buy_and_hold_ETH\": m_eth})\n",
    "\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        plot_series(ec, f\"Equity Curve — PPO ({split['name']})\")\n",
    "        plot_series((ec / ec.cummax()) - 1.0, f\"Drawdown — PPO ({split['name']})\")\n",
    "        plot_series(ec_bench, f\"Equity Curve — Equal-Weight Hold ({split['name']})\")\n",
    "\n",
    "print(\"Done. RESULTS collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for res in RESULTS:\n",
    "    row = {\"split\": res[\"split\"]}\n",
    "    for k, metrics in res.items():\n",
    "        if k == \"split\":\n",
    "            continue\n",
    "        for mname, mval in metrics.items():\n",
    "            row[f\"{k}_{mname}\"] = mval\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "out_json = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.json\")\n",
    "out_csv  = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "print(\"Saved:\", out_json, \"and\", out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
