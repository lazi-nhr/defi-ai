{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for Statistical Arbitrage\n",
    "\n",
    "This notebook demonstrates how to configure and train a reinforcement learning agent for statistical arbitrage using a snapshot model. The agent will learn to identify and exploit statistical arbitrage opportunities in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79612d",
   "metadata": {},
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "The following libraries are required for this notebook. If you haven't installed them yet, you can do so using running the cell below or by using pip install in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b434b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%pip -q install -U numpy pandas pyarrow gdown gymnasium stable-baselines3 torch matplotlib tensorboard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664e80a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration section sets up the parameters for the reinforcement learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DATA\": {\n",
    "        \"forward_fill\": True, # forward fill missing data\n",
    "        \"drop_na_after_ffill\": True, # drop rows with NA values after forward filling\n",
    "        \"cache_dir\": \"./data_cache\", # local cache directory to store downloaded files\n",
    "        \"timestamp_format\": \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"asset_price_format\": \"{ASSET}_close\",\n",
    "        \"pair_feature_format\": \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "        \"timestamp_col\": \"timestamp\",\n",
    "        \"file_names\": {\n",
    "            \"features\": \"historical_pairs_with_spreads\"\n",
    "            },\n",
    "        \"file_ids\": {\n",
    "            \"price_folder_id\": \"1uXEBUyySypdsW_ZqL-RZ3d1bWdIZisij\",\n",
    "            \"ADA_1h\": \"1ydaR3T68ReE_7j5t3wZbj0F-zdRPYoxg\",\n",
    "            \"APT_1h\": \"1CxG9N2bqWPs9fOPOUryYNtHmONXo4SRi\",\n",
    "            \"ARB_1h\": \"136FSMlAW3XHG8WocxxTEcSKiLMUBwWMi\",\n",
    "            \"ATOM_1h\": \"1mhSQgEwRHn3nvu8Qu1ctQGzdW5JuxATR\",\n",
    "            \"BTC_1h\": \"1-sBNQpEFGEpVO3GDFCkSZiV3Iaqp2vB_\",\n",
    "            \"DOGE_1h\": \"14XlkoQMYr8WWecGninAKUavvjB3qNxk0\",\n",
    "            \"DOT_1h\": \"1kCWB4ZZu3FnadbAquTa3Rcdcwkhnq6-s\",\n",
    "            \"ENA_1h\": \"1TYTxexlD24cs7qmhyVoTacX7lqGOsfky\",\n",
    "            \"ETC_1h\": \"1coBd9QiEX03MndMgX5_549mOPyY23ZcI\",\n",
    "            \"ETH_1h\": \"1kj8G1scpFuEYTTXKEUzF9pwgGI2WFFL9\",\n",
    "            \"HBAR_1h\": \"1LVseecBvXKl3Wl9hbPLsROYKR1Gp8zhQ\",\n",
    "            \"LINK_1h\": \"1ZLEraxdV3H8jpf1FmPeVs1ySL7TzMvH5\",\n",
    "            \"LTC_1h\": \"18d3_jD-tuYTQQR2QOwXupckeDgqvAIvx\",\n",
    "            \"NEAR_1h\": \"1PqI2hD2gbDxUaRDPnJpvDNH5wPYv47G6\",\n",
    "            \"SOL_1h\": \"17CjYYSEsTEqBdmm51zGLgmpkslxxjiji\",\n",
    "            \"SUI_1h\": \"1bToOJts-x2Ia48tqXcMs4qFIQ5OV1lAP\",\n",
    "            \"TON_1h\": \"1SARYo5zB6AunG82kw7KGF4Nird3lQ4zB\",\n",
    "            \"TRX_1h\": \"1FlcZo1WRtKFQMbBrsb61Lp3_pplISW4U\",\n",
    "            \"UNI_1h\": \"15L-eKWliyg9MBKuznlZZ-FJzm52Ovt20\",\n",
    "            \"WLD_1h\": \"1XqD1K4-YZzPxYFHKHY3KmKWnnwi3zO20\",\n",
    "            \"XLM_1h\": \"1_3E5-mORLWh3X16Hi0ccHwzVKg5QxoT4\",\n",
    "            \"XRP_1h\": \"1crt2g_t0qpYnaGpcozl35yDeHhd4tmi4\",\n",
    "            \"features\": \"1AWXfCp2egL4d9D1lntAG6cKlzJJ-62c4\"\n",
    "            }\n",
    "    },\n",
    "    \"COINTEGRATION\": {\n",
    "        \"training_window_days\": 2, # number of days to use for training after cointegration period\n",
    "    },\n",
    "    \"ENV\": {\n",
    "        \"include_cash\": True, # include cash as a third asset\n",
    "        \"shorting\": True, # allow shorting\n",
    "        \"trading_window_days\": \"2D\",\n",
    "        \"sliding_window_step\": \"1D\",\n",
    "        \"lookback_window\": 64, # lookback for feature slicing, meaning that the observation for each step will include data from the last 'lookback_window' time steps\n",
    "        \"transaction_costs\": { # transaction costs settings for exchange (e.g. Hyperliquid)\n",
    "            \"commission_bps\": 5.0, # commission in basis points (bps)\n",
    "            \"slippage_bps\": 5.0, # slippage in basis points (bps)\n",
    "        },\n",
    "        \"reward\": {\n",
    "            \"risk_lambda\":0.001 # risk penalty coefficient (lambda), a.k.a. risk aversion factor\n",
    "        },\n",
    "        \"leverage\": {\n",
    "            \"use_leverage\": False,\n",
    "            \"long_cap\": 2.0,  # maximum leverage for long positions\n",
    "            \"short_cap\": 2.0, # maximum leverage for short positions\n",
    "            \"use_asymmetric\": True, # whether to allow different caps for long and short positions\n",
    "        },\n",
    "        \"constraints\": {\n",
    "            \"min_weight\": -2.0, # minimum weight for each asset (for shorting)\n",
    "            \"max_weight\": 2.0,  # maximum weight for each asset (for leverage)\n",
    "            \"sum_to_one\": False # whether weights must sum to one (need to be False for leverage)\n",
    "        },\n",
    "        \"seed\": 42 # random seed for reproducibility\n",
    "    },\n",
    "    \"SPLITS\": { # date splits for training, validation, and testing\n",
    "        \"data_start\":\"2024-09-02\", # start date of the entire dataset\n",
    "        \"data_end\":\"2025-09-02\", # end date of the entire dataset\n",
    "        \"train\":[\"2024-09-02\",\"2025-06-15\"], # training period\n",
    "        \"val\":[\"2025-06-16\",\"2025-07-15\"], # validation period\n",
    "        \"test\":[\"2025-07-16\",\"2025-09-02\"], # testing period\n",
    "        \"walk_forward\": True, # whether to use walk-forward splits (e.g. sliding window)\n",
    "        \"wf_train_span_days\": 180, # training window span in days for walk-forward\n",
    "        \"wf_test_span_days\": 30, # testing window span in days for walk-forward\n",
    "        \"wf_step_days\": 30 # step size in days to move the window forward\n",
    "    },\n",
    "    \"RL\": {\n",
    "        \"timesteps\":10,\n",
    "        \"policy\":\"MlpPolicy\",\n",
    "        \"gamma\":0.99,\n",
    "        \"gae_lambda\":0.95,\n",
    "        \"clip_range\":0.2,\n",
    "        \"n_steps\":1024,\n",
    "        \"batch_size\":256,\n",
    "        \"learning_rate\":3e-4,\n",
    "        \"ent_coef\":0.0,\n",
    "        \"vf_coef\":0.5,\n",
    "        \"max_grad_norm\":0.5\n",
    "    },\n",
    "    \"EVAL\": {\n",
    "        \"plots\":True,\n",
    "        \"reports_dir\":\"./reports\"\n",
    "    },\n",
    "    \"IO\": {\n",
    "        \"models_dir\":\"./models\",\n",
    "        \"tb_logdir\":\"./tb\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568820c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gdown\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import pytz\n",
    "import csv\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from typing import Iterable, Tuple, Dict, List, Set, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c6388",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69053e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set annualization factors for different timeframes\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\n",
    "                 \"5m\":365*24*12,\n",
    "                 \"15m\":365*24*4,\n",
    "                 \"1h\":365*24,\n",
    "                 \"1d\":365}\n",
    "\n",
    "# create directory if it doesn't already exist\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ee23",
   "metadata": {},
   "source": [
    "## Set Computation Device\n",
    "\n",
    "This section sets the computation device for training the model. It checks if a GPU is available and sets it as the device; otherwise, it defaults to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# run on Apple Silicon GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available. Using Apple Silicon GPU.\")\n",
    "\n",
    "# run on CPU (slow)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS are not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63981a61",
   "metadata": {},
   "source": [
    "## Set Seeds\n",
    "\n",
    "This section sets the random seeds for various libraries to ensure that the results are reproducible.\n",
    "\n",
    "Note: It is good practice to type (set data types) for function and method parameters for better code maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775695a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed) # seed for random module\n",
    "    np.random.seed(seed) # seed for numpy module\n",
    "    try:\n",
    "        torch.manual_seed(seed) # seed for torch module\n",
    "        if torch.cuda.is_available(): # seed for CUDA device\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        elif torch.backends.mps.is_available(): # seed for Apple Silicon device\n",
    "            torch.backends.mps.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"]) # set all seeds for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449a3db",
   "metadata": {},
   "source": [
    "## Download Feature Data\n",
    "\n",
    "In this section, we retrieve the .csv file created during feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_name: str, file_id: str, out_dir: str):\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    out_path = os.path.join(out_dir, f\"{file_name}.csv\")  # full output path\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"File {file_name} already exists in cache. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading {file_name} -> {out_path}\")\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        success = gdown.download(url, out_path, quiet=False, use_cookies=False, verify=False)\n",
    "        return success\n",
    "    except Exception as e:\n",
    "        print(f\"Download attempt failed for {file_name}. Error: {str(e)}\")\n",
    "    \n",
    "# donwload features\n",
    "file_name = CONFIG[\"DATA\"][\"file_names\"][\"features\"]\n",
    "file_id = CONFIG[\"DATA\"][\"file_ids\"][\"features\"]\n",
    "cache_dir = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "\n",
    "print(\"Downloading cointegration data...\")\n",
    "success = download_file(file_name, file_id, cache_dir)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151dd54",
   "metadata": {},
   "source": [
    "## Load Feature Data\n",
    "\n",
    "Once the data is downloaded, this section loads the data into a pandas DataFrame for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb39a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_df(\n",
    "    path: str,\n",
    "    parse_timestamp_col: str | None = \"timestamp\",\n",
    "    encoding: str = \"utf-8-sig\",\n",
    "    **read_csv_kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV into a pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Filesystem path to the CSV.\n",
    "    parse_timestamp_col : str | None\n",
    "        If provided and present in the CSV, this column will be parsed to datetime.\n",
    "        Set to None to skip datetime parsing.\n",
    "    **read_csv_kwargs :\n",
    "        Extra arguments passed to `pd.read_csv` (e.g., sep, dtype, usecols).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Try delimiter sniffing first\n",
    "    with open(path, \"r\", encoding=encoding, newline=\"\") as f:\n",
    "        sample = f.read(2048)\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\",\";\",\"|\",\"\\t\"])\n",
    "        sep = dialect.delimiter\n",
    "    except csv.Error:\n",
    "        sep = \",\"  # fallback\n",
    "\n",
    "    # Parse header-only to check for timestamp col presence\n",
    "    head = pd.read_csv(path, sep=sep, encoding=encoding, nrows=0)\n",
    "    if parse_timestamp_col and parse_timestamp_col in head.columns:\n",
    "        read_csv_kwargs = {\n",
    "            **read_csv_kwargs,\n",
    "            \"parse_dates\": [parse_timestamp_col],\n",
    "            \"infer_datetime_format\": True,\n",
    "        }\n",
    "\n",
    "    df = pd.read_csv(path, sep=sep, encoding=encoding, engine=\"python\", **read_csv_kwargs)\n",
    "    return df\n",
    "\n",
    "\n",
    "# load features\n",
    "file_name = CONFIG[\"DATA\"][\"file_names\"][\"features\"]\n",
    "cache_dir = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "file_path = os.path.join(cache_dir, f\"{file_name}.csv\")\n",
    "features_df = load_csv_to_df(file_path, parse_timestamp_col=\"timestamp\")\n",
    "\n",
    "# print dataframe info\n",
    "print(\"Features DataFrame Info:\")\n",
    "print(features_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f09ef",
   "metadata": {},
   "source": [
    "## Identify Feature Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30901f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_assets_features_pairs(\n",
    "    df: pd.DataFrame,\n",
    "    single_asset_format: str,\n",
    "    pair_feature_format: str\n",
    "    ) -> tuple[list[str], list[str], list[tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    Identify:\n",
    "      - Distinct assets from columns matching `single_asset_format`\n",
    "      - Distinct features from columns matching `pair_feature_format`\n",
    "      - Distinct unordered asset pairs found in cross-asset feature columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe whose columns follow the naming conventions.\n",
    "    single_asset_format : str\n",
    "        Format string describing single-asset columns (e.g., \"{ASSET}_close\").\n",
    "    pair_feature_format : str\n",
    "        Format string describing pair-feature columns (e.g., \"{ASSET1}_{ASSET2}_{FEATURE}\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (assets, features, asset_pairs)\n",
    "        assets       : sorted list[str] of unique asset tickers (from single asset columns)\n",
    "        features     : sorted list[str] of unique feature names\n",
    "        asset_pairs  : sorted list[tuple[str, str]] of unique unordered pairs\n",
    "    \"\"\"\n",
    "\n",
    "    # Build regex patterns from format strings\n",
    "    # Example: \"{ASSET}_close\" → r\"^(?P<ASSET>[A-Za-z0-9]+)_close$\"\n",
    "    def format_to_regex(fmt: str) -> re.Pattern:\n",
    "        pattern = re.escape(fmt)\n",
    "        # Replace placeholders like {ASSET}, {ASSET1}, {FEATURE}\n",
    "        pattern = re.sub(r\"\\\\\\{(\\w+)\\\\\\}\", r\"(?P<\\1>[A-Za-z0-9]+)\", pattern)\n",
    "        return re.compile(f\"^{pattern}$\")\n",
    "\n",
    "    single_asset_pattern = format_to_regex(single_asset_format)\n",
    "    pair_feature_pattern = format_to_regex(pair_feature_format)\n",
    "\n",
    "    assets: Set[str] = set()\n",
    "    features: Set[str] = set()\n",
    "    pairs: Set[Tuple[str, str]] = set()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == \"timestamp\":\n",
    "            continue\n",
    "\n",
    "        # Try single-asset pattern\n",
    "        m1 = single_asset_pattern.match(col)\n",
    "        if m1:\n",
    "            assets.add(m1.group(\"ASSET\"))\n",
    "            continue\n",
    "\n",
    "        # Try pair-feature pattern\n",
    "        m2 = pair_feature_pattern.match(col)\n",
    "        if m2:\n",
    "            a1, a2, feat = m2.group(\"ASSET1\"), m2.group(\"ASSET2\"), m2.group(\"FEATURE\")\n",
    "            pair = tuple(sorted((a1, a2)))\n",
    "            assets.update([a1, a2])\n",
    "            pairs.add(pair)\n",
    "            features.add(feat)\n",
    "\n",
    "    return sorted(assets), sorted(features), sorted(pairs)\n",
    "\n",
    "# identify assets, features, and asset pairs\n",
    "single_asset_format = CONFIG[\"DATA\"][\"asset_price_format\"]\n",
    "pair_feature_format = CONFIG[\"DATA\"][\"pair_feature_format\"]\n",
    "assets, features, asset_pairs = identify_assets_features_pairs(\n",
    "    features_df,\n",
    "    single_asset_format,\n",
    "    pair_feature_format\n",
    "    )\n",
    "\n",
    "print(f\"Identified {len(assets)} assets: {assets}\")\n",
    "print(f\"Identified {len(features)} features: {features}\")\n",
    "print(f\"Identified {len(asset_pairs)} asset pairs: {asset_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd4a34",
   "metadata": {},
   "source": [
    "## Build Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f391580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_intervals(\n",
    "    df: pd.DataFrame,\n",
    "    window: pd.Timedelta | str,\n",
    "    step: Optional[pd.Timedelta | str] = None,\n",
    "    timestamp_col: str = \"timestamp\",\n",
    "    include_last_partial: bool = False,\n",
    ") -> list[tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Return fixed-length time intervals over the DataFrame's time span.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain a datetime-like 'timestamp' column or have a DatetimeIndex.\n",
    "    window : pd.Timedelta | str\n",
    "        Size of each window, e.g. '2D', '60min', '15T'.\n",
    "    step : pd.Timedelta | str | None\n",
    "        Step between consecutive window starts. Defaults to `window` (non-overlapping).\n",
    "        Use a smaller step than `window` for sliding/overlapping windows.\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column (ignored if index is a DatetimeIndex).\n",
    "    include_last_partial : bool\n",
    "        If True, include the trailing partial window shorter than `window`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[pd.Timestamp, pd.Timestamp]]\n",
    "        Half-open intervals [start, end).\n",
    "    \"\"\"\n",
    "    W = pd.Timedelta(window)\n",
    "    S = pd.Timedelta(step) if step is not None else W\n",
    "\n",
    "    # Extract, sanitize, and sort timestamps\n",
    "    if timestamp_col in df.columns:\n",
    "        ts = pd.to_datetime(df[timestamp_col]).dropna().sort_values()\n",
    "    elif isinstance(df.index, pd.DatetimeIndex):\n",
    "        ts = pd.Series(df.index).dropna().sort_values()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Timestamp column '{timestamp_col}' not found and index is not DatetimeIndex.\"\n",
    "        )\n",
    "\n",
    "    intervals: list[tuple[pd.Timestamp, pd.Timestamp]] = []\n",
    "    if ts.empty:\n",
    "        return intervals\n",
    "\n",
    "    t_min = ts.iloc[0]\n",
    "    t_max = ts.iloc[-1]\n",
    "    cur = t_min\n",
    "\n",
    "    while cur < t_max:\n",
    "        end = cur + W\n",
    "        if end <= t_max:\n",
    "            intervals.append((cur, end))\n",
    "        elif include_last_partial:\n",
    "            intervals.append((cur, t_max))\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "        cur = cur + S\n",
    "\n",
    "    return intervals\n",
    "\n",
    "# build intervals\n",
    "window = CONFIG[\"ENV\"][\"trading_window_days\"]\n",
    "step = CONFIG[\"ENV\"][\"sliding_window_step\"]\n",
    "timestamp_col = CONFIG[\"DATA\"][\"timestamp_col\"]\n",
    "\n",
    "intervals = build_time_intervals(\n",
    "    features_df,\n",
    "    window,\n",
    "    step,\n",
    "    timestamp_col,\n",
    "    include_last_partial=False\n",
    ")\n",
    "\n",
    "# print interval info\n",
    "print(f\"Built {len(intervals)} time intervals with window={window} and step={step}.\")\n",
    "print(\"First 3 intervals:\")\n",
    "for start, end in intervals[:3]:\n",
    "    print(f\"  {start} to {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febee86",
   "metadata": {},
   "source": [
    "## Identify Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_timeframe_valid(\n",
    "    df: pd.DataFrame,\n",
    "    pair: tuple[str, str],\n",
    "    start: pd.Timestamp,\n",
    "    end: pd.Timestamp,\n",
    "    feature_name: str = \"spread\",\n",
    "    pair_feature_format: str = \"{ASSET1}_{ASSET2}_{FEATURE}\",\n",
    "    timestamp_col: str = \"timestamp\"\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given time frame has complete data for the specified asset pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the data.\n",
    "    pair : tuple[str, str]\n",
    "        Asset pair (asset1, asset2).\n",
    "    start : pd.Timestamp\n",
    "        Start of the time frame (inclusive).\n",
    "    end : pd.Timestamp\n",
    "        End of the time frame (exclusive).\n",
    "    pair_feature_format : str\n",
    "        Format string for pair-feature columns (e.g., \"{ASSET1}_{ASSET2}_{FEATURE}\").\n",
    "    timestamp_col : str\n",
    "        Name of the timestamp column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the time frame is valid (no missing data), False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    asset1, asset2 = pair\n",
    "\n",
    "    # Construct expected column names\n",
    "    feature_col_name = pair_feature_format.format(ASSET1=asset1, ASSET2=asset2, FEATURE=feature_name)\n",
    "\n",
    "    # Filter DataFrame to the specified time frame\n",
    "    mask = (df[timestamp_col] >= start) & (df[timestamp_col] < end)\n",
    "    df_timeframe = df.loc[mask, [timestamp_col, feature_col_name]]\n",
    "\n",
    "    # Check for missing values in any of the required columns\n",
    "    if df_timeframe.isnull().values.any():\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "valid_intervails_per_pair = {}\n",
    "for pair in asset_pairs:\n",
    "    valid_intervals = []\n",
    "    for start, end in intervals:\n",
    "        if is_timeframe_valid(\n",
    "            features_df,\n",
    "            pair,\n",
    "            start,\n",
    "            end,\n",
    "            feature_name=\"spread\",\n",
    "            pair_feature_format=pair_feature_format,\n",
    "            timestamp_col=timestamp_col):\n",
    "\n",
    "            valid_intervals.append((start, end))\n",
    "    valid_intervails_per_pair[pair] = valid_intervals\n",
    "    print(f\"Pair {pair} has {len(valid_intervals)} valid intervals out of {len(intervals)} total intervals.\")\n",
    "\n",
    "print(\"First 3 valid intervals for first 3 pairs:\")\n",
    "for pair in list(valid_intervails_per_pair.keys())[:3]:\n",
    "    print(f\"Pair {pair}:\")\n",
    "    for start, end in valid_intervails_per_pair[pair][:3]:\n",
    "        print(f\"  {start} to {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf36443",
   "metadata": {},
   "source": [
    "## Identify Feature Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cfg = CONFIG[\"ENV\"][\"features\"]\n",
    "price_col = CONFIG[\"DATA\"][\"price_point\"]\n",
    "\n",
    "# based on the columns in the file identify...\n",
    "# ...assets\n",
    "# ...pairs\n",
    "# ...time frames\n",
    "# ...features\n",
    "\n",
    "# function to compute relative strength index (momentum indicator)\n",
    "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff() # compute price changes\n",
    "    up = (delta.clip(lower=0)).ewm(alpha=1/period, adjust=False).mean() # average of upward price changes\n",
    "    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean() # average of downward price changes\n",
    "    rs = up / (down + 1e-12) # relative strength ratio indicates if upward or downward momentum is stronger\n",
    "    rsi = 100 - (100 / (1 + rs)) # converts RS to RSI bounded between 0 and 100\n",
    "    return rsi\n",
    "\n",
    "# function to create features dataframe\n",
    "def make_features(df: pd.DataFrame, price_col: str, vol_window: int, rsi_period: int, volume_change: bool):\n",
    "    out = pd.DataFrame(index=df.index) # initialize output dataframe\n",
    "    out[\"ret\"] = np.log(df[price_col]).diff(1) # log returns\n",
    "    out[\"vol\"] = out[\"ret\"].rolling(vol_window).std().fillna(0.0) # rolling volatility\n",
    "    out[\"rsi\"] = compute_rsi(df[price_col], rsi_period).fillna(50.0) # relative strength index\n",
    "    if \"volume\" in df.columns and volume_change: # log volume change if volume data is available\n",
    "        out[\"volchg\"] = np.log(df[\"volume\"].replace(0, np.nan)).diff().fillna(0.0) # log volume change\n",
    "    else:\n",
    "        out[\"volchg\"] = 0.0 # if no volume data, set to zero\n",
    "    return out\n",
    "\n",
    "features_by_ticker = {}\n",
    "for t in tickers: # create features for each ticker\n",
    "    fdf = make_features(dfs[t], price_col, feat_cfg[\"vol_window\"], feat_cfg[\"rsi_period\"], feat_cfg[\"volume_change\"])\n",
    "    features_by_ticker[t] = fdf # store features in dictionary\n",
    "\n",
    "panel_cols = []\n",
    "for t in tickers: # create multi-index columns\n",
    "    for col in [\"ret\",\"vol\",\"rsi\",\"volchg\"]:\n",
    "        panel_cols.append((t, col))\n",
    "panel = pd.concat([features_by_ticker[t][[\"ret\",\"vol\",\"rsi\",\"volchg\"]] for t in tickers], axis=1) # combine features horizontally\n",
    "panel.columns = pd.MultiIndex.from_tuples(panel_cols, names=[\"ticker\",\"feature\"]) # set multi-index columns\n",
    "panel = panel.dropna() # remove rows with missing values\n",
    "\n",
    "print(panel.tail())\n",
    "panel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1a418",
   "metadata": {},
   "source": [
    "## Feature scaling and state tensor construction\n",
    "\n",
    "This section normalizes the features and constructs the state tensors required for training the reinforcement learning agent. State tensors are multi-dimensional arrays that represent the current state of the environment that the RL agent uses to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = CONFIG[\"ENV\"][\"lookback_window\"] # lookback window \n",
    "normalize = CONFIG[\"ENV\"][\"features\"][\"normalize\"] # normalization method\n",
    "\n",
    "# rolling z-score normalization for features\n",
    "def rolling_zscore(df: pd.DataFrame, window: int = 256) -> pd.DataFrame:\n",
    "    mu = df.rolling(window).mean() # rolling mean\n",
    "    sigma = df.rolling(window).std().replace(0, np.nan) # rolling std dev, replace 0 with NaN to avoid division by zero\n",
    "    z = (df - mu) / (sigma + 1e-12) # z-score normalization\n",
    "    return z.fillna(0.0) # fill NaNs with 0.0 and return z-scored dataframe\n",
    "\n",
    "# build state tensor\n",
    "def build_state_tensor(panel: pd.DataFrame, lookback: int, normalize: bool = False):\n",
    "    normalized = panel.copy()\n",
    "\n",
    "    # organize data by tickers, features, and time\n",
    "    tickers = sorted(panel.columns.unique(level=0)) # ensure consistent order\n",
    "    features = sorted(panel.columns.unique(level=1)) # feature order\n",
    "    times = normalized.index # time index\n",
    "\n",
    "    # create sliding windows of lookback length\n",
    "    # extract feature data for the window and stack into a 3D tensor (tickers x features x lookback)\n",
    "    X, y_ret, inst_vol = [], [], [] # lists to hold state tensors, next returns, and current volatilities\n",
    "    for i in range(lookback, len(times)-1): # iterate over time index in lookback window to have a sliding window with each time point as a step\n",
    "        window_slice = normalized.iloc[i-lookback:i]\n",
    "        frames = []\n",
    "        for t in tickers: # save the windows of each ticker\n",
    "            frames.append(window_slice[t].T.values)\n",
    "        tensor = np.stack(frames, axis=0) # stack into 3D tensor\n",
    "        X.append(tensor) # append tensor to list\n",
    "        nxt = panel.iloc[i+1] # next period returns\n",
    "        y_ret.append(np.array([nxt[(t, \"ret\")] for t in tickers], dtype=float)) # save to list\n",
    "        cur = panel.iloc[i] # current period volatilities\n",
    "        inst_vol.append(np.array([cur[(t, \"vol\")] for t in tickers], dtype=float)) # save to list\n",
    "\n",
    "    X = np.array(X, dtype=np.float32) # convert list of state tensors of tensors to 3D numpy array\n",
    "    y_ret = np.array(y_ret, dtype=np.float32) # convert list of next returns to 2D numpy array\n",
    "    inst_vol = np.array(inst_vol, dtype=np.float32) # convert list of volatilities to 2D numpy array\n",
    "    return X, y_ret, inst_vol, tickers, features, times[lookback+1:]\n",
    "\n",
    "X_all, R_all, VOL_all, TICKER_ORDER, FEAT_ORDER, TIME_INDEX = build_state_tensor(\n",
    "    panel, lookback=lookback, normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"State tensor:\", X_all.shape, \"Returns:\", R_all.shape, \"InstVol:\", VOL_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f60af",
   "metadata": {},
   "source": [
    "## Define Splits and Adjust Timezones\n",
    "\n",
    "This section defines the training and validation splits for the dataset. It ensures that the data is divided appropriately to train the model, validate its performance during training, and test its final performance on unseen data. Also, it adjusts the timezones of the datetime indices to ensure consistency across the dataset. This is necessary in case the data comes from multiple sources with different timezone settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create boolean mask for date slicing\n",
    "def date_slice_mask(times: pd.DatetimeIndex, start: str, end: str):\n",
    "    # convert input dates to UTC timestamps\n",
    "    start_ts = pd.Timestamp(start).tz_localize('UTC')\n",
    "    end_ts = pd.Timestamp(end).tz_localize('UTC')\n",
    "    \n",
    "    # ensure time index is in UTC\n",
    "    if times.tz is None: # if naive, localize to UTC\n",
    "        times = times.tz_localize('UTC')\n",
    "    elif times.tz != pytz.UTC: # if timezone-aware but not UTC, convert to UTC\n",
    "        times = times.tz_convert('UTC')\n",
    "        \n",
    "    return (times >= start_ts) & (times <= end_ts) # return boolean mask for splits later on\n",
    "\n",
    "def create_cointegration_splits(coint_df: pd.DataFrame, price_data_index: pd.DatetimeIndex):\n",
    "    \"\"\"Create training windows based on cointegration periods.\"\"\"\n",
    "    train_masks = []\n",
    "    val_masks = []\n",
    "    test_masks = []\n",
    "    \n",
    "    # Ensure indices are timezone-aware\n",
    "    if price_data_index.tz is None:\n",
    "        price_data_index = price_data_index.tz_localize('UTC')\n",
    "    \n",
    "    # Sort periods chronologically\n",
    "    coint_df = coint_df.sort_values('train_start')\n",
    "    \n",
    "    # Split cointegration periods into train/val/test\n",
    "    split_point1 = int(0.7 * len(coint_df))  # 70% for training\n",
    "    split_point2 = int(0.85 * len(coint_df))  # 15% for validation\n",
    "    \n",
    "    train_periods = coint_df.iloc[:split_point1]\n",
    "    val_periods = coint_df.iloc[split_point1:split_point2]\n",
    "    test_periods = coint_df.iloc[split_point2:]\n",
    "    \n",
    "    # Create masks for each period type\n",
    "    for periods, masks in [(train_periods, train_masks), \n",
    "                          (val_periods, val_masks), \n",
    "                          (test_periods, test_masks)]:\n",
    "        for _, row in periods.iterrows():\n",
    "            mask = (price_data_index >= row['train_start']) & (price_data_index <= row['train_end'])\n",
    "            masks.append(mask)\n",
    "    \n",
    "    # Combine masks for each split with OR operation\n",
    "    train_mask = np.logical_or.reduce(train_masks) if train_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    val_mask = np.logical_or.reduce(val_masks) if val_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    test_mask = np.logical_or.reduce(test_masks) if test_masks else np.zeros(len(price_data_index), dtype=bool)\n",
    "    \n",
    "    print(f\"Training windows: {train_mask.sum()} timesteps\")\n",
    "    print(f\"Validation windows: {val_mask.sum()} timesteps\")\n",
    "    print(f\"Testing windows: {test_mask.sum()} timesteps\")\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "# Create training masks based on cointegration periods\n",
    "train_mask, val_mask, test_mask = create_cointegration_splits(coint_df, TIME_INDEX)\n",
    "\n",
    "# Create splits for training\n",
    "SPLITS = [{\n",
    "    \"name\": \"CointegrationSplit\",\n",
    "    \"train\": train_mask,\n",
    "    \"val\": val_mask,\n",
    "    \"test\": test_mask\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be710c8e",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "This is a custom Gymnasium environment for portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioWeightsEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env):\n",
    "        super().__init__()\n",
    "        self.X = X # state tensor\n",
    "        self.R = R # next period returns\n",
    "        self.VOL = VOL # current period volatilities\n",
    "        self.tickers = tickers # list of assets\n",
    "        self.lookback = lookback # lookback window\n",
    "        self.cfg = cfg_env # environment configuration taken from CONFIG\n",
    "\n",
    "        self.n_assets = len(tickers) # number of assets\n",
    "        self.include_cash = cfg_env[\"include_cash\"] # whether to include cash as an asset\n",
    "        self.shorting = cfg_env[\"shorting\"] # whether shorting is allowed\n",
    "        self.dim_action = self.n_assets + (1 if self.include_cash else 0) # action space dimension: one weight per asset (+1 for cash if included)\n",
    "\n",
    "        obs_dim = self.n_assets * self.X.shape[2] * self.lookback # observation space dimension: tickers x features x lookback\n",
    "        self.observation_space = spaces.Box(low=-5, high=5, shape=(obs_dim,), dtype=np.float32) # needed for Gym to define feature bounds (±5 for z-scored features)\n",
    "        \n",
    "        # Update action space to allow for leveraged positions\n",
    "        if self.shorting:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=-cfg_env[\"leverage\"][\"short_cap\"],\n",
    "                high=cfg_env[\"leverage\"][\"long_cap\"],\n",
    "                shape=(self.dim_action,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        else:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=0.0,\n",
    "                high=cfg_env[\"leverage\"][\"long_cap\"],\n",
    "                shape=(self.dim_action,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4 # commission in decimal form\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4 # slippage in decimal form\n",
    "        self.risk_lambda = cfg_env[\"reward\"][\"risk_lambda\"] # defines risk aversion in reward function\n",
    "\n",
    "        # Leverage settings\n",
    "        self.long_cap = cfg_env[\"leverage\"][\"long_cap\"]\n",
    "        self.short_cap = cfg_env[\"leverage\"][\"short_cap\"]\n",
    "        self.use_asymmetric = cfg_env[\"leverage\"][\"use_asymmetric\"]\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42))\n",
    "\n",
    "    def _to_obs(self, t):\n",
    "        # Get the current window of observations\n",
    "        arr = self.X[t].reshape(-1).astype(np.float32)\n",
    "        return arr\n",
    "    \n",
    "    # project raw actions to asset weights with leverage\n",
    "    def _project_weights(self, a):\n",
    "        if self.use_asymmetric:\n",
    "            # Apply asymmetric leverage caps\n",
    "            w = np.clip(a, -self.short_cap if self.shorting else 0.0, self.long_cap)\n",
    "        else:\n",
    "            # Use symmetric leverage cap\n",
    "            max_leverage = max(self.long_cap, self.short_cap)\n",
    "            w = np.clip(a, -max_leverage if self.shorting else 0.0, max_leverage)\n",
    "        return w\n",
    "\n",
    "    # reset environment to initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0 # reset time index\n",
    "        self.portfolio_value = 1.0 # reset portfolio value\n",
    "        self.w = np.zeros(self.dim_action) # initialize with zero weights\n",
    "        if self.include_cash:\n",
    "            self.w[-1] = 1.0  # start with all cash\n",
    "        obs = self._to_obs(self.t)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        w_target = self._project_weights(action) # get new target weights from raw actions\n",
    "        turnover = np.sum(np.abs(w_target - self.w)) # calculate turnover as sum of absolute weight changes\n",
    "        trading_cost = (self.commission + self.slippage) * turnover # total trading cost based on turnover\n",
    "\n",
    "        asset_w_prev = self.w[:self.n_assets] # previous weights excluding cash\n",
    "        asset_ret = np.dot(asset_w_prev, self.R[self.t]) # calculate asset return based on previous weights and next period returns\n",
    "        inst_vol = np.dot(asset_w_prev, self.VOL[self.t]) # calculate instantaneous volatility based on previous weights and current volatilities\n",
    "\n",
    "        # reward function: asset return minus trading costs and risk penalty\n",
    "        reward = asset_ret - trading_cost - self.risk_lambda * inst_vol\n",
    "\n",
    "        self.portfolio_value *= math.exp(asset_ret - trading_cost) # update portfolio value using log returns\n",
    "\n",
    "        self.w = w_target # update calculated weights for next step\n",
    "        self.t += 1 # move to next time step\n",
    "        terminated = (self.t >= len(self.R)-1) # episode ends if we reach the end of the data\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        obs = np.clip(obs, -5.0, 5.0) # clip observations to previously defined bounds (±5 for z-scored features)\n",
    "        \n",
    "        # Additional info for monitoring leveraged positions\n",
    "        leverage = np.sum(np.abs(self.w[:self.n_assets]))\n",
    "        info = {\n",
    "            \"portfolio_value\": self.portfolio_value, \n",
    "            \"turnover\": turnover, \n",
    "            \"inst_vol\": inst_vol, \n",
    "            \"asset_ret\": asset_ret,\n",
    "            \"total_leverage\": leverage\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_mask(X, R, VOL, mask: np.ndarray):\n",
    "    idx = np.where(mask)[0] # get all indices where mask is True\n",
    "    \n",
    "    # If we have no valid indices, return empty arrays with correct shapes\n",
    "    if len(idx) == 0:\n",
    "        empty_shape_x = list(X.shape)\n",
    "        empty_shape_x[0] = 0\n",
    "        empty_shape_r = list(R.shape)\n",
    "        empty_shape_r[0] = 0\n",
    "        empty_shape_v = list(VOL.shape)\n",
    "        empty_shape_v[0] = 0\n",
    "        return np.zeros(empty_shape_x), np.zeros(empty_shape_r), np.zeros(empty_shape_v)\n",
    "    \n",
    "    return X[idx], R[idx], VOL[idx] # return only the selected slices of data\n",
    "\n",
    "def make_env_from_mask(mask, name=\"env\"):\n",
    "    X_s, R_s, V_s = slice_by_mask(X_all, R_all, VOL_all, mask)\n",
    "    env = PortfolioWeightsEnv(X_s, R_s, V_s, TICKER_ORDER, CONFIG[\"ENV\"][\"lookback_window\"], CONFIG[\"ENV\"])\n",
    "    env = Monitor(env, filename=None)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annualize_factor(sampling: str):\n",
    "    return ANNUALIZATION.get(sampling, 365*24)\n",
    "\n",
    "def compute_metrics(equity_curve: pd.Series, sampling: str, turnover_series: pd.Series = None):\n",
    "    ret = equity_curve.pct_change().dropna()\n",
    "    ann = annualize_factor(sampling)\n",
    "    mu = ret.mean() * ann\n",
    "    sigma = ret.std() * math.sqrt(ann)\n",
    "    sharpe = mu / (sigma + 1e-12)\n",
    "    downside = ret[ret < 0].std() * math.sqrt(ann)\n",
    "    sortino = mu / (downside + 1e-12)\n",
    "    if len(equity_curve) > 1:\n",
    "        # Calculate years based on number of samples and sampling frequency\n",
    "        if isinstance(equity_curve.index, pd.DatetimeIndex):\n",
    "            dt_years = (equity_curve.index[-1] - equity_curve.index[0]).total_seconds() / (365 * 24 * 3600)\n",
    "        else:\n",
    "            # If using RangeIndex, calculate based on sampling frequency\n",
    "            samples = len(equity_curve)\n",
    "            samples_per_year = annualize_factor(sampling)\n",
    "            dt_years = samples / samples_per_year\n",
    "        dt_years = float(dt_years) if float(dt_years) != 0 else 1e-12\n",
    "        cagr = (equity_curve.iloc[-1] / equity_curve.iloc[0]) ** (1/dt_years) - 1\n",
    "    else:\n",
    "        cagr = 0.0\n",
    "    cummax = equity_curve.cummax()\n",
    "    dd = (equity_curve / cummax - 1).min()\n",
    "    maxdd = float(dd)\n",
    "    calmar = mu / (abs(maxdd) + 1e-12)\n",
    "    hit_ratio = (ret > 0).mean()\n",
    "    turnover = turnover_series.mean() if turnover_series is not None and len(turnover_series)>0 else np.nan\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"Sortino\": sortino, \"MaxDrawdown\": maxdd, \"Calmar\": calmar, \"Volatility\": sigma, \"Turnover\": turnover, \"HitRatio\": hit_ratio}\n",
    "\n",
    "def plot_series(series: pd.Series, title: str):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(series.index, series.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_periods(coint_df: pd.DataFrame, price_index: pd.DatetimeIndex):\n",
    "    \"\"\"Plot timeline showing training periods and their overlap.\"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    y = 0\n",
    "    for _, row in coint_df.iterrows():\n",
    "        plt.hlines(y, row['train_start'], row['train_end'], 'blue', alpha=0.3)\n",
    "        y += 1\n",
    "    plt.title(\"Training Periods Timeline\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Period Index\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def backtest_env(env: gym.Env, model=None):\n",
    "    # Get the unwrapped environment\n",
    "    unwrapped = env.unwrapped if hasattr(env, 'unwrapped') else env\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    pv, turns = [], []\n",
    "    leverage = []  # Track leverage over time\n",
    "    \n",
    "    for t in range(len(unwrapped.R)-1):\n",
    "        if model is None:\n",
    "            action = np.ones(unwrapped.dim_action)/unwrapped.dim_action\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        pv.append(info[\"portfolio_value\"])\n",
    "        turns.append(info[\"turnover\"])\n",
    "        leverage.append(info.get(\"total_leverage\", 0))\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    idx = pd.RangeIndex(start=0, stop=len(pv), step=1)\n",
    "    ec = pd.Series(pv, index=idx)\n",
    "    to = pd.Series(turns, index=idx)\n",
    "    lev = pd.Series(leverage, index=idx)\n",
    "    \n",
    "    return ec, to, lev\n",
    "\n",
    "# Plot cointegration training periods\n",
    "print(\"Visualizing training periods timeline...\")\n",
    "plot_training_periods(coint_df, TIME_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(CONFIG[\"IO\"][\"models_dir\"])\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\n=== Training on split: {split['name']} ===\")\n",
    "    train_env = make_env_from_mask(split[\"train\"], name=f\"{split['name']}_train\")\n",
    "    eval_env  = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "\n",
    "    vec_train = DummyVecEnv([lambda: train_env])\n",
    "    vec_eval  = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=CONFIG[\"RL\"][\"policy\"],\n",
    "        env=vec_train,\n",
    "        gamma=CONFIG[\"RL\"][\"gamma\"],\n",
    "        gae_lambda=CONFIG[\"RL\"][\"gae_lambda\"],\n",
    "        clip_range=CONFIG[\"RL\"][\"clip_range\"],\n",
    "        n_steps=CONFIG[\"RL\"][\"n_steps\"],\n",
    "        batch_size=CONFIG[\"RL\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"RL\"][\"learning_rate\"],\n",
    "        ent_coef=CONFIG[\"RL\"][\"ent_coef\"],\n",
    "        vf_coef=CONFIG[\"RL\"][\"vf_coef\"],\n",
    "        max_grad_norm=CONFIG[\"RL\"][\"max_grad_norm\"],\n",
    "        tensorboard_log=CONFIG[\"IO\"][\"tb_logdir\"],\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_eval, \n",
    "        best_model_save_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "        log_path=CONFIG[\"IO\"][\"models_dir\"], \n",
    "        eval_freq=10000,\n",
    "        deterministic=True, \n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=CONFIG[\"RL\"][\"timesteps\"], callback=eval_callback)\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{split['name']}.zip\")\n",
    "    model.save(model_path)\n",
    "    print(\"Saved model:\", model_path)\n",
    "\n",
    "    test_env = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "    ec, to = backtest_env(test_env, model=model)\n",
    "\n",
    "    idx = np.where(split[\"test\"])[0]\n",
    "    R_test = R_all[idx]\n",
    "    ew = np.ones(len(TICKER_ORDER))/len(TICKER_ORDER)\n",
    "    ec_bench = [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        ec_bench.append(ec_bench[-1]*math.exp(np.dot(ew, R_test[i])))\n",
    "    ec_bench = pd.Series(ec_bench, index=ec.index)\n",
    "\n",
    "    bh_btc, bh_eth = [1.0], [1.0]\n",
    "    # Only iterate through the same number of returns as we have in ec.index\n",
    "    for i in range(len(ec.index)-1):\n",
    "        bh_btc.append(bh_btc[-1]*math.exp(R_test[i][0]))\n",
    "        bh_eth.append(bh_eth[-1]*math.exp(R_test[i][1]))\n",
    "    bh_btc = pd.Series(bh_btc, index=ec.index)\n",
    "    bh_eth = pd.Series(bh_eth, index=ec.index)\n",
    "\n",
    "    m_model = compute_metrics(ec, CONFIG[\"DATA\"][\"sampling\"], to)\n",
    "    m_ew    = compute_metrics(ec_bench, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_btc   = compute_metrics(bh_btc, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_eth   = compute_metrics(bh_eth, CONFIG[\"DATA\"][\"sampling\"])\n",
    "\n",
    "    RESULTS.append({\"split\": split[\"name\"], \"model\": m_model, \"equal_weight\": m_ew, \"buy_and_hold_BTC\": m_btc, \"buy_and_hold_ETH\": m_eth})\n",
    "\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        plot_series(ec, f\"Equity Curve — PPO ({split['name']})\")\n",
    "        plot_series((ec / ec.cummax()) - 1.0, f\"Drawdown — PPO ({split['name']})\")\n",
    "        plot_series(ec_bench, f\"Equity Curve — Equal-Weight Hold ({split['name']})\")\n",
    "\n",
    "print(\"Done. RESULTS collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for res in RESULTS:\n",
    "    row = {\"split\": res[\"split\"]}\n",
    "    for k, metrics in res.items():\n",
    "        if k == \"split\":\n",
    "            continue\n",
    "        for mname, mval in metrics.items():\n",
    "            row[f\"{k}_{mname}\"] = mval\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "out_json = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.json\")\n",
    "out_csv  = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "print(\"Saved:\", out_json, \"and\", out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
