{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bb7b54",
   "metadata": {},
   "source": [
    "# Crypto RL Portfolio (PPO) — Continuous Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    \"DATA\": {\n",
    "        \"drive_folder_id\": \"1uXEBUyySypdsW_ZqL-RZ3d1bWdIZisij\",\n",
    "        \"structure\": {\"1d\":\"ohlcv_1d\",\"1h\":\"ohlcv_1h\",\"15m\":\"ohlcv_15m\",\"5m\":\"ohlcv_5m\",\"1m\":\"ohlcv_1m\"},\n",
    "        \"file_pattern\": \"{TICKER}_{FREQ}.parquet\",\n",
    "        \"tickers\": [\"BTC\",\"ETH\"],\n",
    "        \"sampling\": \"1h\",\n",
    "        \"price_point\": \"close\",\n",
    "        \"timezone_in\": \"CET\",\n",
    "        \"timezone_out\": \"UTC\",\n",
    "        \"forward_fill\": True,\n",
    "        \"drop_na_after_ffill\": True,\n",
    "        \"cache_dir\": \"./data_cache\",\n",
    "        \"DOWNLOAD\": {\n",
    "            \"method\": \"folder\",\n",
    "            \"file_ids\": {},\n",
    "            \"download_all\": False\n",
    "        }\n",
    "    },\n",
    "    \"ENV\": {\n",
    "        \"action_space\": \"continuous_weights\",\n",
    "        \"include_cash\": True,\n",
    "        \"shorting\": False,\n",
    "        \"max_leverage\": 1.0,\n",
    "        \"rebalance_interval\": \"1_bar\",\n",
    "        \"episode_length\": {\"mode\":\"steps\",\"value\":2160},\n",
    "        \"lookback_window\": 64,\n",
    "        \"features\": {\n",
    "            \"log_return_window\": 1,\n",
    "            \"vol_window\": 64,\n",
    "            \"rsi_period\": 14,\n",
    "            \"volume_change\": True,\n",
    "            \"scaler\": \"rolling_zscore\"\n",
    "        },\n",
    "        \"transaction_costs\": {\n",
    "            \"commission_bps\": 5.0,\n",
    "            \"slippage_bps\": 5.0,\n",
    "            \"apply_on_rebalance_only\": True\n",
    "        },\n",
    "        \"turnover_penalty\": 0.0,\n",
    "        \"weight_smoothing\": 0.0,\n",
    "        \"reward\": {\"type\":\"log_return_minus_risk\",\"risk_lambda\":0.001},\n",
    "        \"constraints\": {\"min_weight\":0.0,\"max_weight\":1.0,\"sum_to_one\":True},\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"SPLITS\": {\n",
    "        \"data_start\":\"2024-09-02\",\n",
    "        \"data_end\":\"2025-09-02\",\n",
    "        \"train\":[\"2024-09-02\",\"2025-06-15\"],\n",
    "        \"val\":[\"2025-06-16\",\"2025-07-15\"],\n",
    "        \"test\":[\"2025-07-16\",\"2025-09-02\"],\n",
    "        \"walk_forward\": True,\n",
    "        \"wf_train_span_days\": 180,\n",
    "        \"wf_test_span_days\": 30,\n",
    "        \"wf_step_days\": 30\n",
    "    },\n",
    "    \"RL\": {\n",
    "        \"algo\":\"PPO\",\n",
    "        \"timesteps\":8_000_000,\n",
    "        \"device\":\"auto\",\n",
    "        \"policy\":\"MlpPolicy\",\n",
    "        \"gamma\":0.99,\n",
    "        \"gae_lambda\":0.95,\n",
    "        \"clip_range\":0.2,\n",
    "        \"n_steps\":2048,\n",
    "        \"batch_size\":8192,\n",
    "        \"learning_rate\":3e-4,\n",
    "        \"ent_coef\":0.0,\n",
    "        \"vf_coef\":0.5,\n",
    "        \"max_grad_norm\":0.5\n",
    "    },\n",
    "    \"EVAL\": {\n",
    "        \"benchmarks\":[\"equal_weight_hold\",\"buy_and_hold_BTC\",\"buy_and_hold_ETH\"],\n",
    "        \"metrics\":[\"CAGR\",\"Sharpe\",\"Sortino\",\"MaxDrawdown\",\"Calmar\",\"Volatility\",\"Turnover\",\"HitRatio\"],\n",
    "        \"plots\":True,\n",
    "        \"reports_dir\":\"./reports\"\n",
    "    },\n",
    "    \"IO\": {\n",
    "        \"models_dir\":\"./models\",\n",
    "        \"tb_logdir\":\"./tb\",\n",
    "        \"save_best_on_val\": True\n",
    "    }\n",
    "}\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe50027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip -q install -U numpy pandas pyarrow gdown gymnasium stable-baselines3 torch matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4143bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(CONFIG[\"ENV\"][\"seed\"])\n",
    "\n",
    "ANNUALIZATION = {\"1m\":365*24*60,\"5m\":365*24*12,\"15m\":365*24*4,\"1h\":365*24,\"1d\":365}\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gdown\n",
    "\n",
    "ROOT_ID = CONFIG[\"DATA\"][\"drive_folder_id\"]\n",
    "CACHE_DIR = CONFIG[\"DATA\"][\"cache_dir\"]\n",
    "ensure_dir(CACHE_DIR)\n",
    "\n",
    "def download_drive_folder(root_id: str, out_dir: str):\n",
    "    print(\"Mirroring Google Drive folder locally...\")\n",
    "    gdown.download_folder(id=root_id, output=out_dir, quiet=False, use_cookies=False)\n",
    "\n",
    "def targeted_download_by_ids(file_id_map: Dict[str, str], out_dir: str):\n",
    "    ensure_dir(out_dir)\n",
    "    for name, fid in file_id_map.items():\n",
    "        suffix = name if name.endswith(\".parquet\") else f\"{name}.parquet\"\n",
    "        out_path = os.path.join(out_dir, suffix)\n",
    "        print(f\"Downloading {name} -> {out_path}\")\n",
    "        url = f\"https://drive.google.com/uc?id={fid}\"\n",
    "        gdown.download(url, out_path, quiet=False, use_cookies=False)\n",
    "\n",
    "download_method = CONFIG[\"DATA\"][\"DOWNLOAD\"][\"method\"]\n",
    "if download_method == \"file_ids\" and CONFIG[\"DATA\"][\"DOWNLOAD\"][\"file_ids\"]:\n",
    "    targeted_download_by_ids(CONFIG[\"DATA\"][\"DOWNLOAD\"][\"file_ids\"], CACHE_DIR)\n",
    "else:\n",
    "    download_drive_folder(ROOT_ID, CACHE_DIR)\n",
    "\n",
    "print(\"Download step complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling = CONFIG[\"DATA\"][\"sampling\"]\n",
    "subfolder = CONFIG[\"DATA\"][\"structure\"][sampling]\n",
    "pattern_fmt = CONFIG[\"DATA\"][\"file_pattern\"]\n",
    "\n",
    "def find_parquet_path(ticker: str, sampling: str) -> str:\n",
    "    fname = pattern_fmt.format(TICKER=ticker, FREQ=sampling)\n",
    "    candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", subfolder, fname), recursive=True)\n",
    "    if not candidates:\n",
    "        candidates = glob.glob(os.path.join(CACHE_DIR, \"**\", fname), recursive=True)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"Could not find {fname} under {CACHE_DIR}.\")\n",
    "    return candidates[0]\n",
    "\n",
    "def localize_and_align(df: pd.DataFrame, tz_in: str, tz_out: str) -> pd.DataFrame:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp')\n",
    "    if df.index.tz is None:\n",
    "        df = df.tz_localize(tz_in)\n",
    "    df = df.tz_convert(tz_out)\n",
    "    cols = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    return df.sort_index()\n",
    "\n",
    "tz_in, tz_out = CONFIG[\"DATA\"][\"timezone_in\"], CONFIG[\"DATA\"][\"timezone_out\"]\n",
    "tickers = CONFIG[\"DATA\"][\"tickers\"]\n",
    "\n",
    "dfs = {}\n",
    "for t in tickers:\n",
    "    pth = find_parquet_path(t, sampling)\n",
    "    tmp = pd.read_parquet(pth)\n",
    "    tmp = localize_and_align(tmp, tz_in, tz_out)\n",
    "    if CONFIG[\"DATA\"][\"forward_fill\"]:\n",
    "        tmp = tmp.ffill()\n",
    "    if CONFIG[\"DATA\"][\"drop_na_after_ffill\"]:\n",
    "        tmp = tmp.dropna()\n",
    "    dfs[t] = tmp\n",
    "\n",
    "common_index = None\n",
    "for t, df in dfs.items():\n",
    "    common_index = df.index if common_index is None else common_index.intersection(df.index)\n",
    "for t in tickers:\n",
    "    dfs[t] = dfs[t].reindex(common_index).dropna()\n",
    "\n",
    "print({t: dfs[t].shape for t in tickers})\n",
    "dfs[\"BTC\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    up = (delta.clip(lower=0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = up / (down + 1e-12)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def make_features(df: pd.DataFrame, price_col: str, vol_window: int, rsi_period: int, volume_change: bool):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[\"ret\"] = np.log(df[price_col]).diff(1)\n",
    "    out[\"vol\"] = out[\"ret\"].rolling(vol_window).std().fillna(0.0)\n",
    "    out[\"rsi\"] = compute_rsi(df[price_col], rsi_period).fillna(50.0)\n",
    "    if \"volume\" in df.columns and volume_change:\n",
    "        out[\"volchg\"] = np.log(df[\"volume\"].replace(0, np.nan)).diff().fillna(0.0)\n",
    "    else:\n",
    "        out[\"volchg\"] = 0.0\n",
    "    return out\n",
    "\n",
    "feat_cfg = CONFIG[\"ENV\"][\"features\"]\n",
    "lookback = CONFIG[\"ENV\"][\"lookback_window\"]\n",
    "price_col = CONFIG[\"DATA\"][\"price_point\"]\n",
    "\n",
    "features_by_ticker = {}\n",
    "for t in tickers:\n",
    "    fdf = make_features(dfs[t], price_col, feat_cfg[\"vol_window\"], feat_cfg[\"rsi_period\"], feat_cfg[\"volume_change\"])\n",
    "    features_by_ticker[t] = fdf\n",
    "\n",
    "panel_cols = []\n",
    "for t in tickers:\n",
    "    for col in [\"ret\",\"vol\",\"rsi\",\"volchg\"]:\n",
    "        panel_cols.append((t, col))\n",
    "panel = pd.concat([features_by_ticker[t][[\"ret\",\"vol\",\"rsi\",\"volchg\"]] for t in tickers], axis=1)\n",
    "panel.columns = pd.MultiIndex.from_tuples(panel_cols, names=[\"ticker\",\"feature\"])\n",
    "panel = panel.dropna()\n",
    "\n",
    "print(panel.tail())\n",
    "panel.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_zscore(df: pd.DataFrame, window: int = 256) -> pd.DataFrame:\n",
    "    mu = df.rolling(window).mean()\n",
    "    sigma = df.rolling(window).std().replace(0, np.nan)\n",
    "    z = (df - mu) / (sigma + 1e-12)\n",
    "    return z.fillna(0.0)\n",
    "\n",
    "def build_state_tensor(panel: pd.DataFrame, lookback: int, scaler: str = \"rolling_zscore\"):\n",
    "    if scaler == \"rolling_zscore\":\n",
    "        scaled = panel.groupby(level=1, axis=1).apply(lambda g: rolling_zscore(g, window=max(lookback*2, 256)))\n",
    "        scaled.columns = panel.columns\n",
    "    else:\n",
    "        scaled = panel.copy()\n",
    "\n",
    "    tickers = sorted({c[0] for c in scaled.columns})\n",
    "    features = sorted({c[1] for c in scaled.columns})\n",
    "    times = scaled.index\n",
    "\n",
    "    X, y_ret, inst_vol = [], [], []\n",
    "    for i in range(lookback, len(times)-1):\n",
    "        window_slice = scaled.iloc[i-lookback:i]\n",
    "        frames = []\n",
    "        for t in tickers:\n",
    "            frames.append(window_slice[t].T.values)\n",
    "        tensor = np.stack(frames, axis=0)\n",
    "        X.append(tensor)\n",
    "        nxt = panel.iloc[i+1]\n",
    "        y_ret.append(np.array([nxt[(t, \"ret\")] for t in tickers], dtype=float))\n",
    "        cur = panel.iloc[i]\n",
    "        inst_vol.append(np.array([cur[(t, \"vol\")] for t in tickers], dtype=float))\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y_ret = np.array(y_ret, dtype=np.float32)\n",
    "    inst_vol = np.array(inst_vol, dtype=np.float32)\n",
    "    return X, y_ret, inst_vol, tickers, features, times[lookback+1:]\n",
    "\n",
    "X_all, R_all, VOL_all, TICKER_ORDER, FEAT_ORDER, TIME_INDEX = build_state_tensor(\n",
    "    panel, lookback=CONFIG[\"ENV\"][\"lookback_window\"], scaler=CONFIG[\"ENV\"][\"features\"][\"scaler\"]\n",
    ")\n",
    "print(\"State tensor:\", X_all.shape, \"Returns:\", R_all.shape, \"InstVol:\", VOL_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def date_slice_mask(times: pd.DatetimeIndex, start: str, end: str):\n",
    "    return (times >= pd.Timestamp(start, tz=CONFIG[\"DATA\"][\"timezone_out\"])) & (times <= pd.Timestamp(end, tz=CONFIG[\"DATA\"][\"timezone_out\"]))\n",
    "\n",
    "def build_splits(times: pd.DatetimeIndex, cfg: dict):\n",
    "    s = CONFIG[\"SPLITS\"]\n",
    "    if not s[\"walk_forward\"]:\n",
    "        m_train = date_slice_mask(times, s[\"train\"][0], s[\"train\"][1])\n",
    "        m_val   = date_slice_mask(times, s[\"val\"][0], s[\"val\"][1])\n",
    "        m_test  = date_slice_mask(times, s[\"test\"][0], s[\"test\"][1])\n",
    "        return [{\"name\":\"BaseSplit\",\"train\":m_train,\"val\":m_val,\"test\":m_test}]\n",
    "    else:\n",
    "        start = pd.Timestamp(s[\"data_start\"], tz=CONFIG[\"DATA\"][\"timezone_out\"])\n",
    "        end   = pd.Timestamp(s[\"data_end\"], tz=CONFIG[\"DATA\"][\"timezone_out\"])\n",
    "        spans = []\n",
    "        cur_train_start = start\n",
    "        while True:\n",
    "            train_end = cur_train_start + timedelta(days=s[\"wf_train_span_days\"])\n",
    "            test_end  = train_end + timedelta(days=s[\"wf_test_span_days\"])\n",
    "            if test_end > end:\n",
    "                break\n",
    "            m_train = (times >= cur_train_start) & (times <= train_end)\n",
    "            m_val   = (times > train_end) & (times <= train_end)\n",
    "            m_test  = (times > train_end) & (times <= test_end)\n",
    "            spans.append({\"name\":f\"WF_{cur_train_start.date()}_{(test_end.date())}\",\"train\":m_train,\"val\":m_val,\"test\":m_test})\n",
    "            cur_train_start = cur_train_start + timedelta(days=s[\"wf_step_days\"])\n",
    "        return spans\n",
    "\n",
    "SPLITS = build_splits(TIME_INDEX, CONFIG[\"SPLITS\"])\n",
    "print(f\"Built {len(SPLITS)} split(s). Example:\", SPLITS[0][\"name\"] if SPLITS else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PortfolioWeightsEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, X, R, VOL, tickers, lookback, cfg_env, sampling=\"1h\"):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.R = R\n",
    "        self.VOL = VOL\n",
    "        self.tickers = tickers\n",
    "        self.lookback = lookback\n",
    "        self.cfg = cfg_env\n",
    "        self.sampling = sampling\n",
    "\n",
    "        self.n_assets = len(tickers)\n",
    "        self.include_cash = cfg_env[\"include_cash\"]\n",
    "        self.dim_action = self.n_assets + (1 if self.include_cash else 0)\n",
    "\n",
    "        obs_dim = self.n_assets * self.X.shape[2] * self.lookback\n",
    "        self.observation_space = spaces.Box(low=-10, high=10, shape=(obs_dim,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.dim_action,), dtype=np.float32)\n",
    "\n",
    "        self.commission = cfg_env[\"transaction_costs\"][\"commission_bps\"] / 1e4\n",
    "        self.slippage = cfg_env[\"transaction_costs\"][\"slippage_bps\"] / 1e4\n",
    "        self.apply_on_rebalance_only = cfg_env[\"transaction_costs\"][\"apply_on_rebalance_only\"]\n",
    "        self.risk_lambda = cfg_env[\"reward\"][\"risk_lambda\"]\n",
    "\n",
    "        self.min_w = cfg_env[\"constraints\"][\"min_weight\"]\n",
    "        self.max_w = cfg_env[\"constraints\"][\"max_weight\"]\n",
    "        self.sum_to_one = cfg_env[\"constraints\"][\"sum_to_one\"]\n",
    "\n",
    "        self.reset(seed=cfg_env.get(\"seed\", 42))\n",
    "\n",
    "    def _to_obs(self, t):\n",
    "        arr = self.X[t].reshape(-1).astype(np.float32)\n",
    "        return arr\n",
    "\n",
    "    def _project_weights(self, a):\n",
    "        if self.sum_to_one:\n",
    "            expo = np.exp(a - np.max(a))\n",
    "            w = expo / np.sum(expo)\n",
    "        else:\n",
    "            w = np.clip(a, self.min_w, self.max_w)\n",
    "        if not self.cfg[\"shorting\"]:\n",
    "            w = np.clip(w, 0.0, 1.0)\n",
    "        return w\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0\n",
    "        self.portfolio_value = 1.0\n",
    "        self.w = np.ones(self.dim_action) / self.dim_action\n",
    "        obs = self._to_obs(self.t)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        w_target = self._project_weights(action)\n",
    "        turnover = np.sum(np.abs(w_target - self.w))\n",
    "        trading_cost = (self.commission + self.slippage) * turnover\n",
    "\n",
    "        asset_w_prev = self.w[:self.n_assets]\n",
    "        asset_ret = np.dot(asset_w_prev, self.R[self.t])\n",
    "        inst_vol = np.dot(asset_w_prev, self.VOL[self.t])\n",
    "\n",
    "        reward = asset_ret - trading_cost - self.risk_lambda * inst_vol\n",
    "\n",
    "        self.portfolio_value *= math.exp(asset_ret - trading_cost)\n",
    "\n",
    "        self.w = w_target\n",
    "        self.t += 1\n",
    "        terminated = (self.t >= len(self.R)-1)\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._to_obs(self.t) if not terminated else self._to_obs(self.t-1)\n",
    "        info = {\"portfolio_value\": self.portfolio_value, \"turnover\": turnover, \"inst_vol\": inst_vol, \"asset_ret\": asset_ret}\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slice_by_mask(X, R, VOL, mask: np.ndarray):\n",
    "    idx = np.where(mask)[0]\n",
    "    return X[idx], R[idx], VOL[idx]\n",
    "\n",
    "def make_env_from_mask(mask, name=\"env\"):\n",
    "    X_s, R_s, V_s = slice_by_mask(X_all, R_all, VOL_all, mask)\n",
    "    env = PortfolioWeightsEnv(X_s, R_s, V_s, TICKER_ORDER, CONFIG[\"ENV\"][\"lookback_window\"], CONFIG[\"ENV\"], sampling=CONFIG[\"DATA\"][\"sampling\"])\n",
    "    env = Monitor(env, filename=None)\n",
    "    return env\n",
    "\n",
    "print(\"Env factory ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def annualize_factor(sampling: str):\n",
    "    return ANNUALIZATION.get(sampling, 365*24)\n",
    "\n",
    "def compute_metrics(equity_curve: pd.Series, sampling: str, turnover_series: pd.Series = None):\n",
    "    ret = equity_curve.pct_change().dropna()\n",
    "    ann = annualize_factor(sampling)\n",
    "    mu = ret.mean() * ann\n",
    "    sigma = ret.std() * math.sqrt(ann)\n",
    "    sharpe = mu / (sigma + 1e-12)\n",
    "    downside = ret[ret < 0].std() * math.sqrt(ann)\n",
    "    sortino = mu / (downside + 1e-12)\n",
    "    if len(equity_curve) > 1:\n",
    "        dt_years = (equity_curve.index[-1] - equity_curve.index[0]) / pd.Timedelta(days=365)\n",
    "        dt_years = float(dt_years) if float(dt_years) != 0 else 1e-12\n",
    "        cagr = (equity_curve.iloc[-1] / equity_curve.iloc[0]) ** (1/dt_years) - 1\n",
    "    else:\n",
    "        cagr = 0.0\n",
    "    cummax = equity_curve.cummax()\n",
    "    dd = (equity_curve / cummax - 1).min()\n",
    "    maxdd = float(dd)\n",
    "    calmar = mu / (abs(maxdd) + 1e-12)\n",
    "    hit_ratio = (ret > 0).mean()\n",
    "    turnover = turnover_series.mean() if turnover_series is not None and len(turnover_series)>0 else np.nan\n",
    "    return {\"CAGR\": cagr, \"Sharpe\": sharpe, \"Sortino\": sortino, \"MaxDrawdown\": maxdd, \"Calmar\": calmar, \"Volatility\": sigma, \"Turnover\": turnover, \"HitRatio\": hit_ratio}\n",
    "\n",
    "def plot_series(series: pd.Series, title: str):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(series.index, series.values)\n",
    "    plt.title(title); plt.xlabel(\"Time\"); plt.ylabel(\"Value\"); plt.show()\n",
    "\n",
    "def backtest_env(env: PortfolioWeightsEnv, model=None):\n",
    "    obs, _ = env.reset()\n",
    "    pv, turns = [], []\n",
    "    for t in range(len(env.R)-1):\n",
    "        if model is None:\n",
    "            action = np.ones(env.dim_action)/env.dim_action\n",
    "        else:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        pv.append(info[\"portfolio_value\"])\n",
    "        turns.append(info[\"turnover\"])\n",
    "        if done:\n",
    "            break\n",
    "    idx = pd.RangeIndex(start=0, stop=len(pv), step=1)\n",
    "    ec = pd.Series(pv, index=idx)\n",
    "    to = pd.Series(turns, index=idx)\n",
    "    return ec, to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensure_dir(CONFIG[\"IO\"][\"models_dir\"])\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\n=== Training on split: {split['name']} ===\")\n",
    "    train_env = make_env_from_mask(split[\"train\"], name=f\"{split['name']}_train\")\n",
    "    eval_env  = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "\n",
    "    vec_train = DummyVecEnv([lambda: train_env])\n",
    "    vec_eval  = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=CONFIG[\"RL\"][\"policy\"],\n",
    "        env=vec_train,\n",
    "        gamma=CONFIG[\"RL\"][\"gamma\"],\n",
    "        gae_lambda=CONFIG[\"RL\"][\"gae_lambda\"],\n",
    "        clip_range=CONFIG[\"RL\"][\"clip_range\"],\n",
    "        n_steps=CONFIG[\"RL\"][\"n_steps\"],\n",
    "        batch_size=CONFIG[\"RL\"][\"batch_size\"],\n",
    "        learning_rate=CONFIG[\"RL\"][\"learning_rate\"],\n",
    "        ent_coef=CONFIG[\"RL\"][\"ent_coef\"],\n",
    "        vf_coef=CONFIG[\"RL\"][\"vf_coef\"],\n",
    "        max_grad_norm=CONFIG[\"RL\"][\"max_grad_norm\"],\n",
    "        tensorboard_log=CONFIG[\"IO\"][\"tb_logdir\"],\n",
    "        device=CONFIG[\"RL\"][\"device\"],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(vec_eval, best_model_save_path=CONFIG[\"IO\"][\"models_dir\"],\n",
    "                                 log_path=CONFIG[\"IO\"][\"models_dir\"], eval_freq=10000,\n",
    "                                 deterministic=True, render=False)\n",
    "    model.learn(total_timesteps=CONFIG[\"RL\"][\"timesteps\"], callback=eval_callback)\n",
    "    model_path = os.path.join(CONFIG[\"IO\"][\"models_dir\"], f\"ppo_{split['name']}.zip\")\n",
    "    model.save(model_path)\n",
    "    print(\"Saved model:\", model_path)\n",
    "\n",
    "    test_env = make_env_from_mask(split[\"test\"], name=f\"{split['name']}_test\")\n",
    "    ec, to = backtest_env(test_env, model=model)\n",
    "\n",
    "    idx = np.where(split[\"test\"])[0]\n",
    "    R_test = R_all[idx]\n",
    "    ew = np.ones(len(TICKER_ORDER))/len(TICKER_ORDER)\n",
    "    ec_bench = [1.0]\n",
    "    for r in R_test[:-1]:\n",
    "        ec_bench.append(ec_bench[-1]*math.exp(np.dot(ew, r)))\n",
    "    ec_bench = pd.Series(ec_bench, index=ec.index)\n",
    "\n",
    "    bh_btc, bh_eth = [1.0], [1.0]\n",
    "    for r in R_test[:-1]:\n",
    "        bh_btc.append(bh_btc[-1]*math.exp(r[0]))\n",
    "        bh_eth.append(bh_eth[-1]*math.exp(r[1]))\n",
    "    bh_btc = pd.Series(bh_btc, index=ec.index)\n",
    "    bh_eth = pd.Series(bh_eth, index=ec.index)\n",
    "\n",
    "    m_model = compute_metrics(ec, CONFIG[\"DATA\"][\"sampling\"], to)\n",
    "    m_ew    = compute_metrics(ec_bench, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_btc   = compute_metrics(bh_btc, CONFIG[\"DATA\"][\"sampling\"])\n",
    "    m_eth   = compute_metrics(bh_eth, CONFIG[\"DATA\"][\"sampling\"])\n",
    "\n",
    "    RESULTS.append({\"split\": split[\"name\"], \"model\": m_model, \"equal_weight\": m_ew, \"buy_and_hold_BTC\": m_btc, \"buy_and_hold_ETH\": m_eth})\n",
    "\n",
    "    if CONFIG[\"EVAL\"][\"plots\"]:\n",
    "        plot_series(ec, f\"Equity Curve — PPO ({split['name']})\")\n",
    "        plot_series((ec / ec.cummax()) - 1.0, f\"Drawdown — PPO ({split['name']})\")\n",
    "        plot_series(ec_bench, f\"Equity Curve — Equal-Weight Hold ({split['name']})\")\n",
    "\n",
    "print(\"Done. RESULTS collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for res in RESULTS:\n",
    "    row = {\"split\": res[\"split\"]}\n",
    "    for k, metrics in res.items():\n",
    "        if k == \"split\":\n",
    "            continue\n",
    "        for mname, mval in metrics.items():\n",
    "            row[f\"{k}_{mname}\"] = mval\n",
    "    rows.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ensure_dir(CONFIG[\"EVAL\"][\"reports_dir\"])\n",
    "out_json = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.json\")\n",
    "out_csv  = os.path.join(CONFIG[\"EVAL\"][\"reports_dir\"], f\"metrics_{ts}.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(RESULTS, f, indent=2)\n",
    "print(\"Saved:\", out_json, \"and\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2a785",
   "metadata": {},
   "source": [
    "**Notes:** Adjust fees for Hyperliquid; consider a separate validation set; extend features and risk controls as needed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
